dictionary_from_Sentinel_to_Mask2former = {
    "model.vision_encoder.fc.weight":"stem.fc.weight ",
    "model.vision_encoder.fc.bias":"stem.fc.bias",

    #STEM
    "model.vision_encoder.conv1.weight":"stem.conv1.weight",
    "model.vision_encoder.bn1.running_mean":"stem.conv1.norm.running_mean",
    "model.vision_encoder.bn1.running_var":"stem.conv1.norm.running_var",
    "model.vision_encoder.bn1.weight":"stem.conv1.norm.weight",
    "model.vision_encoder.bn1.bias ":"stem.conv1.norm.bias",
    
    # RES2 Block Layer0
    "model.vision_encoder.layer1.0.conv1.weight":"res2.0.conv1.weight",
    "model.vision_encoder.layer1.0.bn1.running_mean":"res2.0.conv1.norm.running_mean",
    "model.vision_encoder.layer1.0.bn1.running_var":"res2.0.conv1.norm.running_var",
    "model.vision_encoder.layer1.0.bn1.weight":"res2.0.conv1.norm.weight",
    "model.vision_encoder.layer1.0.bn1.bias":"res2.0.conv1.norm.bias",
    "model.vision_encoder.layer1.0.conv2.weight":"res2.0.conv2.weight",
    "model.vision_encoder.layer1.0.bn2.running_mean":"res2.0.conv2.norm.running_mean",
    "model.vision_encoder.layer1.0.bn2.running_var":"res2.0.conv2.norm.running_var",
    "model.vision_encoder.layer1.0.bn2.weight":"res2.0.conv2.norm.weight",
    "model.vision_encoder.layer1.0.bn2.bias":"res2.0.conv2.norm.bias",
    "model.vision_encoder.layer1.0.conv3.weight":"res2.0.conv3.weight",
    "model.vision_encoder.layer1.0.bn3.running_mean":"res2.0.conv3.norm.running_mean",
    "model.vision_encoder.layer1.0.bn3.running_var":"res2.0.conv3.norm.running_var",
    "model.vision_encoder.layer1.0.bn3.weight":"res2.0.conv3.norm.weight",
    "model.vision_encoder.layer1.0.bn3.bias":"res2.0.conv3.norm.bias",
    "model.vision_encoder.layer1.0.downsample.0.weight":"res2.0.shortcut.weight",
    "model.vision_encoder.layer1.0.downsample.1.running_mean":"res2.0.shortcut.norm.running_mean",
    "model.vision_encoder.layer1.0.downsample.1.running_var":"res2.0.shortcut.norm.running_var",
    "model.vision_encoder.layer1.0.downsample.1.weight":"res2.0.shortcut.norm.weight",
    "model.vision_encoder.layer1.0.downsample.1.bias":"res2.0.shortcut.norm.bias",

    # RES2 Block Layer1
    "model.vision_encoder.layer1.1.conv1.weight":"res2.1.conv1.weight",
    "model.vision_encoder.layer1.1.bn1.running_mean":"res2.1.conv1.norm.running_mean",
    "model.vision_encoder.layer1.1.bn1.running_var":"res2.1.conv1.norm.running_var",
    "model.vision_encoder.layer1.1.bn1.weight":"res2.1.conv1.norm.weight",
    "model.vision_encoder.layer1.1.bn1.bias":"res2.1.conv1.norm.bias",
    "model.vision_encoder.layer1.1.conv2.weight":"res2.1.conv2.weight",
    "model.vision_encoder.layer1.1.bn2.running_mean":"res2.1.conv2.norm.running_mean",
    "model.vision_encoder.layer1.1.bn2.running_var":"res2.1.conv2.norm.running_var",
    "model.vision_encoder.layer1.1.bn2.weight":"res2.1.conv2.norm.weight",
    "model.vision_encoder.layer1.1.bn2.bias":"res2.1.conv2.norm.bias",
    "model.vision_encoder.layer1.1.conv3.weight":"res2.1.conv3.weight",
    "model.vision_encoder.layer1.1.bn3.running_mean":"res2.1.conv3.norm.running_mean",
    "model.vision_encoder.layer1.1.bn3.running_var":"res2.1.conv3.norm.running_var",
    "model.vision_encoder.layer1.1.bn3.weight":"res2.1.conv3.norm.weight",
    "model.vision_encoder.layer1.1.bn3.bias":"res2.1.conv3.norm.bias",

    # RES2 Block Layer2
    "model.vision_encoder.layer1.2.conv1.weight":"res2.2.conv1.weight",
    "model.vision_encoder.layer1.2.bn1.running_mean":"res2.2.conv1.norm.running_mean",
    "model.vision_encoder.layer1.2.bn1.running_var":"res2.2.conv1.norm.running_var",
    "model.vision_encoder.layer1.2.bn1.weight":"res2.2.conv1.norm.weight",
    "model.vision_encoder.layer1.2.bn1.bias":"res2.2.conv1.norm.bias",
    "model.vision_encoder.layer1.2.conv2.weight":"res2.2.conv2.weight",
    "model.vision_encoder.layer1.2.bn2.running_mean":"res2.2.conv2.norm.running_mean",
    "model.vision_encoder.layer1.2.bn2.running_var":"res2.2.conv2.norm.running_var",
    "model.vision_encoder.layer1.2.bn2.weight":"res2.2.conv2.norm.weight",
    "model.vision_encoder.layer1.2.bn2.bias":"res2.2.conv2.norm.bias",
    "model.vision_encoder.layer1.2.conv3.weight":"res2.2.conv3.weight",
    "model.vision_encoder.layer1.2.bn3.running_mean":"res2.2.conv3.norm.running_mean",
    "model.vision_encoder.layer1.2.bn3.running_var":"res2.2.conv3.norm.running_var",
    "model.vision_encoder.layer1.2.bn3.weight":"res2.2.conv3.norm.weight",
    "model.vision_encoder.layer1.2.bn3.bias":"res2.2.conv3.norm.bias",

    # RES3 Block Layer0
    "model.vision_encoder.layer2.0.conv1.weight":"res3.0.conv1.weight",
    "model.vision_encoder.layer2.0.bn1.running_mean":"res3.0.conv1.norm.running_mean",
    "model.vision_encoder.layer2.0.bn1.running_var":"res3.0.conv1.norm.running_var",
    "model.vision_encoder.layer2.0.bn1.weight":"res3.0.conv1.norm.weight",
    "model.vision_encoder.layer2.0.bn1.bias":"res3.0.conv1.norm.bias",
    "model.vision_encoder.layer2.0.conv2.weight":"res3.0.conv2.weight",
    "model.vision_encoder.layer2.0.bn2.running_mean":"res3.0.conv2.norm.running_mean",
    "model.vision_encoder.layer2.0.bn2.running_var":"res3.0.conv2.norm.running_var",
    "model.vision_encoder.layer2.0.bn2.weight":"res3.0.conv2.norm.weight",
    "model.vision_encoder.layer2.0.bn2.bias":"res3.0.conv2.norm.bias",
    "model.vision_encoder.layer2.0.conv3.weight":"res3.0.conv3.weight",
    "model.vision_encoder.layer2.0.bn3.running_mean":"res3.0.conv3.norm.running_mean",
    "model.vision_encoder.layer2.0.bn3.running_var":"res3.0.conv3.norm.running_var",
    "model.vision_encoder.layer2.0.bn3.weight":"res3.0.conv3.norm.weight",
    "model.vision_encoder.layer2.0.bn3.bias":"res3.0.conv3.norm.bias",
    "model.vision_encoder.layer2.0.downsample.0.weight":"res3.0.shortcut.weight",
    "model.vision_encoder.layer2.0.downsample.1.running_mean":"res3.0.shortcut.norm.running_mean",
    "model.vision_encoder.layer2.0.downsample.1.running_var":"res3.0.shortcut.norm.running_var",
    "model.vision_encoder.layer2.0.downsample.1.weight":"res3.0.shortcut.norm.weight",
    "model.vision_encoder.layer2.0.downsample.1.bias":"res3.0.shortcut.norm.bias",

    # RES3 Block Layer1
    "model.vision_encoder.layer2.1.conv1.weight":"res3.1.conv1.weight",
    "model.vision_encoder.layer2.1.bn1.running_mean":"res3.1.conv1.norm.running_mean",
    "model.vision_encoder.layer2.1.bn1.running_var":"res3.1.conv1.norm.running_var",
    "model.vision_encoder.layer2.1.bn1.weight":"res3.1.conv1.norm.weight",
    "model.vision_encoder.layer2.1.bn1.bias":"res3.1.conv1.norm.bias",
    "model.vision_encoder.layer2.1.conv2.weight":"res3.1.conv2.weight",
    "model.vision_encoder.layer2.1.bn2.running_mean":"res3.1.conv2.norm.running_mean",
    "model.vision_encoder.layer2.1.bn2.running_var":"res3.1.conv2.norm.running_var",
    "model.vision_encoder.layer2.1.bn2.weight":"res3.1.conv2.norm.weight",
    "model.vision_encoder.layer2.1.bn2.bias":"res3.1.conv2.norm.bias",
    "model.vision_encoder.layer2.1.conv3.weight":"res3.1.conv3.weight",
    "model.vision_encoder.layer2.1.bn3.running_mean":"res3.1.conv3.norm.running_mean",
    "model.vision_encoder.layer2.1.bn3.running_var":"res3.1.conv3.norm.running_var",
    "model.vision_encoder.layer2.1.bn3.weight":"res3.1.conv3.norm.weight",
    "model.vision_encoder.layer2.1.bn3.bias":"res3.1.conv3.norm.bias",

    # RES3 Block Layer2
    "model.vision_encoder.layer2.2.conv1.weight":"res3.2.conv1.weight",
    "model.vision_encoder.layer2.2.bn1.running_mean":"res3.2.conv1.norm.running_mean",
    "model.vision_encoder.layer2.2.bn1.running_var":"res3.2.conv1.norm.running_var",
    "model.vision_encoder.layer2.2.bn1.weight":"res3.2.conv1.norm.weight",
    "model.vision_encoder.layer2.2.bn1.bias":"res3.2.conv1.norm.bias",
    "model.vision_encoder.layer2.2.conv2.weight":"res3.2.conv2.weight",
    "model.vision_encoder.layer2.2.bn2.running_mean":"res3.2.conv2.norm.running_mean",
    "model.vision_encoder.layer2.2.bn2.running_var":"res3.2.conv2.norm.running_var",
    "model.vision_encoder.layer2.2.bn2.weight":"res3.2.conv2.norm.weight",
    "model.vision_encoder.layer2.2.bn2.bias":"res3.2.conv2.norm.bias",
    "model.vision_encoder.layer2.2.conv3.weight":"res3.2.conv3.weight",
    "model.vision_encoder.layer2.2.bn3.running_mean":"res3.2.conv3.norm.running_mean",
    "model.vision_encoder.layer2.2.bn3.running_var":"res3.2.conv3.norm.running_var",
    "model.vision_encoder.layer2.2.bn3.weight":"res3.2.conv3.norm.weight",
    "model.vision_encoder.layer2.2.bn3.bias":"res3.2.conv3.norm.bias",

    # RES3 Block Layer3
    "model.vision_encoder.layer2.3.conv1.weight":"res3.3.conv1.weight",
    "model.vision_encoder.layer2.3.bn1.running_mean":"res3.3.conv1.norm.running_mean",
    "model.vision_encoder.layer2.3.bn1.running_var":"res3.3.conv1.norm.running_var",
    "model.vision_encoder.layer2.3.bn1.weight":"res3.3.conv1.norm.weight",
    "model.vision_encoder.layer2.3.bn1.bias":"res3.3.conv1.norm.bias",
    "model.vision_encoder.layer2.3.conv2.weight":"res3.3.conv2.weight",
    "model.vision_encoder.layer2.3.bn2.running_mean":"res3.3.conv2.norm.running_mean",
    "model.vision_encoder.layer2.3.bn2.running_var":"res3.3.conv2.norm.running_var",
    "model.vision_encoder.layer2.3.bn2.weight":"res3.3.conv2.norm.weight",
    "model.vision_encoder.layer2.3.bn2.bias":"res3.3.conv2.norm.bias",
    "model.vision_encoder.layer2.3.conv3.weight":"res3.3.conv3.weight",
    "model.vision_encoder.layer2.3.bn3.running_mean":"res3.3.conv3.norm.running_mean",
    "model.vision_encoder.layer2.3.bn3.running_var":"res3.3.conv3.norm.running_var",
    "model.vision_encoder.layer2.3.bn3.weight":"res3.3.conv3.norm.weight",
    "model.vision_encoder.layer2.3.bn3.bias":"res3.3.conv3.norm.bias",

    # RES4 Block Layer0
    "model.vision_encoder.layer3.0.conv1.weight":"res4.0.conv1.weight",
    "model.vision_encoder.layer3.0.bn1.running_mean":"res4.0.conv1.norm.running_mean",
    "model.vision_encoder.layer3.0.bn1.running_var":"res4.0.conv1.norm.running_var",
    "model.vision_encoder.layer3.0.bn1.weight":"res4.0.conv1.norm.weight",
    "model.vision_encoder.layer3.0.bn1.bias":"res4.0.conv1.norm.bias",
    "model.vision_encoder.layer3.0.conv2.weight":"res4.0.conv2.weight",
    "model.vision_encoder.layer3.0.bn2.running_mean":"res4.0.conv2.norm.running_mean",
    "model.vision_encoder.layer3.0.bn2.running_var":"res4.0.conv2.norm.running_var",
    "model.vision_encoder.layer3.0.bn2.weight":"res4.0.conv2.norm.weight",
    "model.vision_encoder.layer3.0.bn2.bias":"res4.0.conv2.norm.bias",
    "model.vision_encoder.layer3.0.conv3.weight":"res4.0.conv3.weight",
    "model.vision_encoder.layer3.0.bn3.running_mean":"res4.0.conv3.norm.running_mean",
    "model.vision_encoder.layer3.0.bn3.running_var":"res4.0.conv3.norm.running_var",
    "model.vision_encoder.layer3.0.bn3.weight":"res4.0.conv3.norm.weight",
    "model.vision_encoder.layer3.0.bn3.bias":"res4.0.conv3.norm.bias",
    "model.vision_encoder.layer3.0.downsample.0.weight":"res4.0.shortcut.weight",
    "model.vision_encoder.layer3.0.downsample.1.running_mean":"res4.0.shortcut.norm.running_mean",
    "model.vision_encoder.layer3.0.downsample.1.running_var":"res4.0.shortcut.norm.running_var",
    "model.vision_encoder.layer3.0.downsample.1.weight":"res4.0.shortcut.norm.weight",
    "model.vision_encoder.layer3.0.downsample.1.bias":"res4.0.shortcut.norm.bias",

    # RES4 Block Layer1
    "model.vision_encoder.layer3.1.conv1.weight":"res4.1.conv1.weight",
    "model.vision_encoder.layer3.1.bn1.running_mean":"res4.1.conv1.norm.running_mean",
    "model.vision_encoder.layer3.1.bn1.running_var":"res4.1.conv1.norm.running_var",
    "model.vision_encoder.layer3.1.bn1.weight":"res4.1.conv1.norm.weight",
    "model.vision_encoder.layer3.1.bn1.bias":"res4.1.conv1.norm.bias",
    "model.vision_encoder.layer3.1.conv2.weight":"res4.1.conv2.weight",
    "model.vision_encoder.layer3.1.bn2.running_mean":"res4.1.conv2.norm.running_mean",
    "model.vision_encoder.layer3.1.bn2.running_var":"res4.1.conv2.norm.running_var",
    "model.vision_encoder.layer3.1.bn2.weight":"res4.1.conv2.norm.weight",
    "model.vision_encoder.layer3.1.bn2.bias":"res4.1.conv2.norm.bias",
    "model.vision_encoder.layer3.1.conv3.weight":"res4.1.conv3.weight",
    "model.vision_encoder.layer3.1.bn3.running_mean":"res4.1.conv3.norm.running_mean",
    "model.vision_encoder.layer3.1.bn3.running_var":"res4.1.conv3.norm.running_var",
    "model.vision_encoder.layer3.1.bn3.weight":"res4.1.conv3.norm.weight",
    "model.vision_encoder.layer3.1.bn3.bias":"res4.1.conv3.norm.bias",

    # RES4 Block Layer2
    "model.vision_encoder.layer3.2.conv1.weight":"res4.2.conv1.weight",
    "model.vision_encoder.layer3.2.bn1.running_mean":"res4.2.conv1.norm.running_mean",
    "model.vision_encoder.layer3.2.bn1.running_var":"res4.2.conv1.norm.running_var",
    "model.vision_encoder.layer3.2.bn1.weight":"res4.2.conv1.norm.weight",
    "model.vision_encoder.layer3.2.bn1.bias":"res4.2.conv1.norm.bias",
    "model.vision_encoder.layer3.2.conv2.weight":"res4.2.conv2.weight",
    "model.vision_encoder.layer3.2.bn2.running_mean":"res4.2.conv2.norm.running_mean",
    "model.vision_encoder.layer3.2.bn2.running_var":"res4.2.conv2.norm.running_var",
    "model.vision_encoder.layer3.2.bn2.weight":"res4.2.conv2.norm.weight",
    "model.vision_encoder.layer3.2.bn2.bias":"res4.2.conv2.norm.bias",
    "model.vision_encoder.layer3.2.conv3.weight":"res4.2.conv3.weight",
    "model.vision_encoder.layer3.2.bn3.running_mean":"res4.2.conv3.norm.running_mean",
    "model.vision_encoder.layer3.2.bn3.running_var":"res4.2.conv3.norm.running_var",
    "model.vision_encoder.layer3.2.bn3.weight":"res4.2.conv3.norm.weight",
    "model.vision_encoder.layer3.2.bn3.bias":"res4.2.conv3.norm.bias",

    # RES4 Block Layer3
    "model.vision_encoder.layer3.3.conv1.weight":"res4.3.conv1.weight",
    "model.vision_encoder.layer3.3.bn1.running_mean":"res4.3.conv1.norm.running_mean",
    "model.vision_encoder.layer3.3.bn1.running_var":"res4.3.conv1.norm.running_var",
    "model.vision_encoder.layer3.3.bn1.weight":"res4.3.conv1.norm.weight",
    "model.vision_encoder.layer3.3.bn1.bias":"res4.3.conv1.norm.bias",
    "model.vision_encoder.layer3.3.conv2.weight":"res4.3.conv2.weight",
    "model.vision_encoder.layer3.3.bn2.running_mean":"res4.3.conv2.norm.running_mean",
    "model.vision_encoder.layer3.3.bn2.running_var":"res4.3.conv2.norm.running_var",
    "model.vision_encoder.layer3.3.bn2.weight":"res4.3.conv2.norm.weight",
    "model.vision_encoder.layer3.3.bn2.bias":"res4.3.conv2.norm.bias",
    "model.vision_encoder.layer3.3.conv3.weight":"res4.3.conv3.weight",
    "model.vision_encoder.layer3.3.bn3.running_mean":"res4.3.conv3.norm.running_mean",
    "model.vision_encoder.layer3.3.bn3.running_var":"res4.3.conv3.norm.running_var",
    "model.vision_encoder.layer3.3.bn3.weight":"res4.3.conv3.norm.weight",
    "model.vision_encoder.layer3.3.bn3.bias":"res4.3.conv3.norm.bias",

    # RES4 Block Layer4
    "model.vision_encoder.layer3.4.conv1.weight":"res4.4.conv1.weight",
    "model.vision_encoder.layer3.4.bn1.running_mean":"res4.4.conv1.norm.running_mean",
    "model.vision_encoder.layer3.4.bn1.running_var":"res4.4.conv1.norm.running_var",
    "model.vision_encoder.layer3.4.bn1.weight":"res4.4.conv1.norm.weight",
    "model.vision_encoder.layer3.4.bn1.bias":"res4.4.conv1.norm.bias",
    "model.vision_encoder.layer3.4.conv2.weight":"res4.4.conv2.weight",
    "model.vision_encoder.layer3.4.bn2.running_mean":"res4.4.conv2.norm.running_mean",
    "model.vision_encoder.layer3.4.bn2.running_var":"res4.4.conv2.norm.running_var",
    "model.vision_encoder.layer3.4.bn2.weight":"res4.4.conv2.norm.weight",
    "model.vision_encoder.layer3.4.bn2.bias":"res4.4.conv2.norm.bias",
    "model.vision_encoder.layer3.4.conv3.weight":"res4.4.conv3.weight",
    "model.vision_encoder.layer3.4.bn3.running_mean":"res4.4.conv3.norm.running_mean",
    "model.vision_encoder.layer3.4.bn3.running_var":"res4.4.conv3.norm.running_var",
    "model.vision_encoder.layer3.4.bn3.weight":"res4.4.conv3.norm.weight",
    "model.vision_encoder.layer3.4.bn3.bias":"res4.4.conv3.norm.bias",

    # RES4 Block Layer5
    "model.vision_encoder.layer3.5.conv1.weight":"res4.5.conv1.weight",
    "model.vision_encoder.layer3.5.bn1.running_mean":"res4.5.conv1.norm.running_mean",
    "model.vision_encoder.layer3.5.bn1.running_var":"res4.5.conv1.norm.running_var",
    "model.vision_encoder.layer3.5.bn1.weight":"res4.5.conv1.norm.weight",
    "model.vision_encoder.layer3.5.bn1.bias":"res4.5.conv1.norm.bias",
    "model.vision_encoder.layer3.5.conv2.weight":"res4.5.conv2.weight",
    "model.vision_encoder.layer3.5.bn2.running_mean":"res4.5.conv2.norm.running_mean",
    "model.vision_encoder.layer3.5.bn2.running_var":"res4.5.conv2.norm.running_var",
    "model.vision_encoder.layer3.5.bn2.weight":"res4.5.conv2.norm.weight",
    "model.vision_encoder.layer3.5.bn2.bias":"res4.5.conv2.norm.bias",
    "model.vision_encoder.layer3.5.conv3.weight":"res4.5.conv3.weight",
    "model.vision_encoder.layer3.5.bn3.running_mean":"res4.5.conv3.norm.running_mean",
    "model.vision_encoder.layer3.5.bn3.running_var":"res4.5.conv3.norm.running_var",
    "model.vision_encoder.layer3.5.bn3.weight":"res4.5.conv3.norm.weight",
    "model.vision_encoder.layer3.5.bn3.bias":"res4.5.conv3.norm.bias",

    # RES5 Block Layer0
    "model.vision_encoder.layer4.0.conv1.weight":"res5.0.conv1.weight",
    "model.vision_encoder.layer4.0.bn1.running_mean":"res5.0.conv1.norm.running_mean",
    "model.vision_encoder.layer4.0.bn1.running_var":"res5.0.conv1.norm.running_var",
    "model.vision_encoder.layer4.0.bn1.weight":"res5.0.conv1.norm.weight",
    "model.vision_encoder.layer4.0.bn1.bias":"res5.0.conv1.norm.bias",
    "model.vision_encoder.layer4.0.conv2.weight":"res5.0.conv2.weight",
    "model.vision_encoder.layer4.0.bn2.running_mean":"res5.0.conv2.norm.running_mean",
    "model.vision_encoder.layer4.0.bn2.running_var":"res5.0.conv2.norm.running_var",
    "model.vision_encoder.layer4.0.bn2.weight":"res5.0.conv2.norm.weight",
    "model.vision_encoder.layer4.0.bn2.bias":"res5.0.conv2.norm.bias",
    "model.vision_encoder.layer4.0.conv3.weight":"res5.0.conv3.weight",
    "model.vision_encoder.layer4.0.bn3.running_mean":"res5.0.conv3.norm.running_mean",
    "model.vision_encoder.layer4.0.bn3.running_var":"res5.0.conv3.norm.running_var",
    "model.vision_encoder.layer4.0.bn3.weight":"res5.0.conv3.norm.weight",
    "model.vision_encoder.layer4.0.bn3.bias":"res5.0.conv3.norm.bias",
    "model.vision_encoder.layer4.0.downsample.0.weight":"res5.0.shortcut.weight",
    "model.vision_encoder.layer4.0.downsample.1.running_mean":"res5.0.shortcut.norm.running_mean",
    "model.vision_encoder.layer4.0.downsample.1.running_var":"res5.0.shortcut.norm.running_var",
    "model.vision_encoder.layer4.0.downsample.1.weight":"res5.0.shortcut.norm.weight",
    "model.vision_encoder.layer4.0.downsample.1.bias":"res5.0.shortcut.norm.bias",

    # RES5 Block Layer1
    "model.vision_encoder.layer4.1.conv1.weight":"res5.1.conv1.weight",
    "model.vision_encoder.layer4.1.bn1.running_mean":"res5.1.conv1.norm.running_mean",
    "model.vision_encoder.layer4.1.bn1.running_var":"res5.1.conv1.norm.running_var",
    "model.vision_encoder.layer4.1.bn1.weight":"res5.1.conv1.norm.weight",
    "model.vision_encoder.layer4.1.bn1.bias":"res5.1.conv1.norm.bias",
    "model.vision_encoder.layer4.1.conv2.weight":"res5.1.conv2.weight",
    "model.vision_encoder.layer4.1.bn2.running_mean":"res5.1.conv2.norm.running_mean",
    "model.vision_encoder.layer4.1.bn2.running_var":"res5.1.conv2.norm.running_var",
    "model.vision_encoder.layer4.1.bn2.weight":"res5.1.conv2.norm.weight",
    "model.vision_encoder.layer4.1.bn2.bias":"res5.1.conv2.norm.bias",
    "model.vision_encoder.layer4.1.conv3.weight":"res5.1.conv3.weight",
    "model.vision_encoder.layer4.1.bn3.running_mean":"res5.1.conv3.norm.running_mean",
    "model.vision_encoder.layer4.1.bn3.running_var":"res5.1.conv3.norm.running_var",
    "model.vision_encoder.layer4.1.bn3.weight":"res5.1.conv3.norm.weight",
    "model.vision_encoder.layer4.1.bn3.bias":"res5.1.conv3.norm.bias",

    # RES5 Block Layer2
    "model.vision_encoder.layer4.2.conv1.weight":"res5.2.conv1.weight",
    "model.vision_encoder.layer4.2.bn1.running_mean":"res5.2.conv1.norm.running_mean",
    "model.vision_encoder.layer4.2.bn1.running_var":"res5.2.conv1.norm.running_var",
    "model.vision_encoder.layer4.2.bn1.weight":"res5.2.conv1.norm.weight",
    "model.vision_encoder.layer4.2.bn1.bias":"res5.2.conv1.norm.bias",
    "model.vision_encoder.layer4.2.conv2.weight":"res5.2.conv2.weight",
    "model.vision_encoder.layer4.2.bn2.running_mean":"res5.2.conv2.norm.running_mean",
    "model.vision_encoder.layer4.2.bn2.running_var":"res5.2.conv2.norm.running_var",
    "model.vision_encoder.layer4.2.bn2.weight":"res5.2.conv2.norm.weight",
    "model.vision_encoder.layer4.2.bn2.bias":"res5.2.conv2.norm.bias",
    "model.vision_encoder.layer4.2.conv3.weight":"res5.2.conv3.weight",
    "model.vision_encoder.layer4.2.bn3.running_mean":"res5.2.conv3.norm.running_mean",
    "model.vision_encoder.layer4.2.bn3.running_var":"res5.2.conv3.norm.running_var",
    "model.vision_encoder.layer4.2.bn3.weight":"res5.2.conv3.norm.weight",
    "model.vision_encoder.layer4.2.bn3.bias":"res5.2.conv3.norm.bias",
}