Big Earth Resnet Weights Sentinel 2 (10 bands)
Weight name                                                       Shape
=====================================================================================
model.vision_encoder.bn1.bias                                     torch.Size([64])
model.vision_encoder.bn1.num_batches_tracked                      torch.Size([])
model.vision_encoder.bn1.running_mean                             torch.Size([64])
model.vision_encoder.bn1.running_var                              torch.Size([64])
model.vision_encoder.bn1.weight                                   torch.Size([64])
model.vision_encoder.conv1.weight                                 torch.Size([64, 10, 7, 7])

model.vision_encoder.fc.bias                                      torch.Size([19])
model.vision_encoder.fc.weight                                    torch.Size([19, 2048])


model.vision_encoder.layer1.0.bn1.bias                            torch.Size([64])
model.vision_encoder.layer1.0.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer1.0.bn1.running_mean                    torch.Size([64])
model.vision_encoder.layer1.0.bn1.running_var                     torch.Size([64])
model.vision_encoder.layer1.0.bn1.weight                          torch.Size([64])
model.vision_encoder.layer1.0.bn2.bias                            torch.Size([64])
model.vision_encoder.layer1.0.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer1.0.bn2.running_mean                    torch.Size([64])
model.vision_encoder.layer1.0.bn2.running_var                     torch.Size([64])
model.vision_encoder.layer1.0.bn2.weight                          torch.Size([64])
model.vision_encoder.layer1.0.bn3.bias                            torch.Size([256])
model.vision_encoder.layer1.0.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer1.0.bn3.running_mean                    torch.Size([256])
model.vision_encoder.layer1.0.bn3.running_var                     torch.Size([256])
model.vision_encoder.layer1.0.bn3.weight                          torch.Size([256])
model.vision_encoder.layer1.0.conv1.weight                        torch.Size([64, 64, 1, 1])
model.vision_encoder.layer1.0.conv2.weight                        torch.Size([64, 64, 3, 3])
model.vision_encoder.layer1.0.conv3.weight                        torch.Size([256, 64, 1, 1])
model.vision_encoder.layer1.0.downsample.0.weight                 torch.Size([256, 64, 1, 1])
model.vision_encoder.layer1.0.downsample.1.bias                   torch.Size([256])
model.vision_encoder.layer1.0.downsample.1.num_batches_tracked    torch.Size([])
model.vision_encoder.layer1.0.downsample.1.running_mean           torch.Size([256])
model.vision_encoder.layer1.0.downsample.1.running_var            torch.Size([256])
model.vision_encoder.layer1.0.downsample.1.weight                 torch.Size([256])


model.vision_encoder.layer1.1.bn1.bias                            torch.Size([64])
model.vision_encoder.layer1.1.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer1.1.bn1.running_mean                    torch.Size([64])
model.vision_encoder.layer1.1.bn1.running_var                     torch.Size([64])
model.vision_encoder.layer1.1.bn1.weight                          torch.Size([64])
model.vision_encoder.layer1.1.bn2.bias                            torch.Size([64])
model.vision_encoder.layer1.1.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer1.1.bn2.running_mean                    torch.Size([64])
model.vision_encoder.layer1.1.bn2.running_var                     torch.Size([64])
model.vision_encoder.layer1.1.bn2.weight                          torch.Size([64])
model.vision_encoder.layer1.1.bn3.bias                            torch.Size([256])
model.vision_encoder.layer1.1.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer1.1.bn3.running_mean                    torch.Size([256])
model.vision_encoder.layer1.1.bn3.running_var                     torch.Size([256])
model.vision_encoder.layer1.1.bn3.weight                          torch.Size([256])
model.vision_encoder.layer1.1.conv1.weight                        torch.Size([64, 256, 1, 1])
model.vision_encoder.layer1.1.conv2.weight                        torch.Size([64, 64, 3, 3])
model.vision_encoder.layer1.1.conv3.weight                        torch.Size([256, 64, 1, 1])


model.vision_encoder.layer1.2.bn1.bias                            torch.Size([64])
model.vision_encoder.layer1.2.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer1.2.bn1.running_mean                    torch.Size([64])
model.vision_encoder.layer1.2.bn1.running_var                     torch.Size([64])
model.vision_encoder.layer1.2.bn1.weight                          torch.Size([64])
model.vision_encoder.layer1.2.bn2.bias                            torch.Size([64])
model.vision_encoder.layer1.2.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer1.2.bn2.running_mean                    torch.Size([64])
model.vision_encoder.layer1.2.bn2.running_var                     torch.Size([64])
model.vision_encoder.layer1.2.bn2.weight                          torch.Size([64])
model.vision_encoder.layer1.2.bn3.bias                            torch.Size([256])
model.vision_encoder.layer1.2.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer1.2.bn3.running_mean                    torch.Size([256])
model.vision_encoder.layer1.2.bn3.running_var                     torch.Size([256])
model.vision_encoder.layer1.2.bn3.weight                          torch.Size([256])
model.vision_encoder.layer1.2.conv1.weight                        torch.Size([64, 256, 1, 1])
model.vision_encoder.layer1.2.conv2.weight                        torch.Size([64, 64, 3, 3])
model.vision_encoder.layer1.2.conv3.weight                        torch.Size([256, 64, 1, 1])



model.vision_encoder.layer2.0.bn1.bias                            torch.Size([128])
model.vision_encoder.layer2.0.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.0.bn1.running_mean                    torch.Size([128])
model.vision_encoder.layer2.0.bn1.running_var                     torch.Size([128])
model.vision_encoder.layer2.0.bn1.weight                          torch.Size([128])
model.vision_encoder.layer2.0.bn2.bias                            torch.Size([128])
model.vision_encoder.layer2.0.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.0.bn2.running_mean                    torch.Size([128])
model.vision_encoder.layer2.0.bn2.running_var                     torch.Size([128])
model.vision_encoder.layer2.0.bn2.weight                          torch.Size([128])
model.vision_encoder.layer2.0.bn3.bias                            torch.Size([512])
model.vision_encoder.layer2.0.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.0.bn3.running_mean                    torch.Size([512])
model.vision_encoder.layer2.0.bn3.running_var                     torch.Size([512])
model.vision_encoder.layer2.0.bn3.weight                          torch.Size([512])
model.vision_encoder.layer2.0.conv1.weight                        torch.Size([128, 256, 1, 1])
model.vision_encoder.layer2.0.conv2.weight                        torch.Size([128, 128, 3, 3])
model.vision_encoder.layer2.0.conv3.weight                        torch.Size([512, 128, 1, 1])
model.vision_encoder.layer2.0.downsample.0.weight                 torch.Size([512, 256, 1, 1])
model.vision_encoder.layer2.0.downsample.1.bias                   torch.Size([512])
model.vision_encoder.layer2.0.downsample.1.num_batches_tracked    torch.Size([])
model.vision_encoder.layer2.0.downsample.1.running_mean           torch.Size([512])
model.vision_encoder.layer2.0.downsample.1.running_var            torch.Size([512])
model.vision_encoder.layer2.0.downsample.1.weight                 torch.Size([512])




model.vision_encoder.layer2.1.bn1.bias                            torch.Size([128])
model.vision_encoder.layer2.1.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.1.bn1.running_mean                    torch.Size([128])
model.vision_encoder.layer2.1.bn1.running_var                     torch.Size([128])
model.vision_encoder.layer2.1.bn1.weight                          torch.Size([128])
model.vision_encoder.layer2.1.bn2.bias                            torch.Size([128])
model.vision_encoder.layer2.1.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.1.bn2.running_mean                    torch.Size([128])
model.vision_encoder.layer2.1.bn2.running_var                     torch.Size([128])
model.vision_encoder.layer2.1.bn2.weight                          torch.Size([128])
model.vision_encoder.layer2.1.bn3.bias                            torch.Size([512])
model.vision_encoder.layer2.1.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.1.bn3.running_mean                    torch.Size([512])
model.vision_encoder.layer2.1.bn3.running_var                     torch.Size([512])
model.vision_encoder.layer2.1.bn3.weight                          torch.Size([512])
model.vision_encoder.layer2.1.conv1.weight                        torch.Size([128, 512, 1, 1])
model.vision_encoder.layer2.1.conv2.weight                        torch.Size([128, 128, 3, 3])
model.vision_encoder.layer2.1.conv3.weight                        torch.Size([512, 128, 1, 1])


model.vision_encoder.layer2.2.bn1.bias                            torch.Size([128])
model.vision_encoder.layer2.2.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.2.bn1.running_mean                    torch.Size([128])
model.vision_encoder.layer2.2.bn1.running_var                     torch.Size([128])
model.vision_encoder.layer2.2.bn1.weight                          torch.Size([128])
model.vision_encoder.layer2.2.bn2.bias                            torch.Size([128])
model.vision_encoder.layer2.2.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.2.bn2.running_mean                    torch.Size([128])
model.vision_encoder.layer2.2.bn2.running_var                     torch.Size([128])
model.vision_encoder.layer2.2.bn2.weight                          torch.Size([128])
model.vision_encoder.layer2.2.bn3.bias                            torch.Size([512])
model.vision_encoder.layer2.2.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.2.bn3.running_mean                    torch.Size([512])
model.vision_encoder.layer2.2.bn3.running_var                     torch.Size([512])
model.vision_encoder.layer2.2.bn3.weight                          torch.Size([512])
model.vision_encoder.layer2.2.conv1.weight                        torch.Size([128, 512, 1, 1])
model.vision_encoder.layer2.2.conv2.weight                        torch.Size([128, 128, 3, 3])
model.vision_encoder.layer2.2.conv3.weight                        torch.Size([512, 128, 1, 1])



model.vision_encoder.layer2.3.bn1.bias                            torch.Size([128])
model.vision_encoder.layer2.3.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.3.bn1.running_mean                    torch.Size([128])
model.vision_encoder.layer2.3.bn1.running_var                     torch.Size([128])
model.vision_encoder.layer2.3.bn1.weight                          torch.Size([128])
model.vision_encoder.layer2.3.bn2.bias                            torch.Size([128])
model.vision_encoder.layer2.3.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.3.bn2.running_mean                    torch.Size([128])
model.vision_encoder.layer2.3.bn2.running_var                     torch.Size([128])
model.vision_encoder.layer2.3.bn2.weight                          torch.Size([128])
model.vision_encoder.layer2.3.bn3.bias                            torch.Size([512])
model.vision_encoder.layer2.3.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer2.3.bn3.running_mean                    torch.Size([512])
model.vision_encoder.layer2.3.bn3.running_var                     torch.Size([512])
model.vision_encoder.layer2.3.bn3.weight                          torch.Size([512])
model.vision_encoder.layer2.3.conv1.weight                        torch.Size([128, 512, 1, 1])
model.vision_encoder.layer2.3.conv2.weight                        torch.Size([128, 128, 3, 3])
model.vision_encoder.layer2.3.conv3.weight                        torch.Size([512, 128, 1, 1])



model.vision_encoder.layer3.0.bn1.bias                            torch.Size([256])
model.vision_encoder.layer3.0.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.0.bn1.running_mean                    torch.Size([256])
model.vision_encoder.layer3.0.bn1.running_var                     torch.Size([256])
model.vision_encoder.layer3.0.bn1.weight                          torch.Size([256])
model.vision_encoder.layer3.0.bn2.bias                            torch.Size([256])
model.vision_encoder.layer3.0.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.0.bn2.running_mean                    torch.Size([256])
model.vision_encoder.layer3.0.bn2.running_var                     torch.Size([256])
model.vision_encoder.layer3.0.bn2.weight                          torch.Size([256])
model.vision_encoder.layer3.0.bn3.bias                            torch.Size([1024])
model.vision_encoder.layer3.0.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.0.bn3.running_mean                    torch.Size([1024])
model.vision_encoder.layer3.0.bn3.running_var                     torch.Size([1024])
model.vision_encoder.layer3.0.bn3.weight                          torch.Size([1024])
model.vision_encoder.layer3.0.conv1.weight                        torch.Size([256, 512, 1, 1])
model.vision_encoder.layer3.0.conv2.weight                        torch.Size([256, 256, 3, 3])
model.vision_encoder.layer3.0.conv3.weight                        torch.Size([1024, 256, 1, 1])
model.vision_encoder.layer3.0.downsample.0.weight                 torch.Size([1024, 512, 1, 1])
model.vision_encoder.layer3.0.downsample.1.bias                   torch.Size([1024])
model.vision_encoder.layer3.0.downsample.1.num_batches_tracked    torch.Size([])
model.vision_encoder.layer3.0.downsample.1.running_mean           torch.Size([1024])
model.vision_encoder.layer3.0.downsample.1.running_var            torch.Size([1024])
model.vision_encoder.layer3.0.downsample.1.weight                 torch.Size([1024])


model.vision_encoder.layer3.1.bn1.bias                            torch.Size([256])
model.vision_encoder.layer3.1.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.1.bn1.running_mean                    torch.Size([256])
model.vision_encoder.layer3.1.bn1.running_var                     torch.Size([256])
model.vision_encoder.layer3.1.bn1.weight                          torch.Size([256])
model.vision_encoder.layer3.1.bn2.bias                            torch.Size([256])
model.vision_encoder.layer3.1.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.1.bn2.running_mean                    torch.Size([256])
model.vision_encoder.layer3.1.bn2.running_var                     torch.Size([256])
model.vision_encoder.layer3.1.bn2.weight                          torch.Size([256])
model.vision_encoder.layer3.1.bn3.bias                            torch.Size([1024])
model.vision_encoder.layer3.1.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.1.bn3.running_mean                    torch.Size([1024])
model.vision_encoder.layer3.1.bn3.running_var                     torch.Size([1024])
model.vision_encoder.layer3.1.bn3.weight                          torch.Size([1024])
model.vision_encoder.layer3.1.conv1.weight                        torch.Size([256, 1024, 1, 1])
model.vision_encoder.layer3.1.conv2.weight                        torch.Size([256, 256, 3, 3])
model.vision_encoder.layer3.1.conv3.weight                        torch.Size([1024, 256, 1, 1])

model.vision_encoder.layer3.2.bn1.bias                            torch.Size([256])
model.vision_encoder.layer3.2.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.2.bn1.running_mean                    torch.Size([256])
model.vision_encoder.layer3.2.bn1.running_var                     torch.Size([256])
model.vision_encoder.layer3.2.bn1.weight                          torch.Size([256])
model.vision_encoder.layer3.2.bn2.bias                            torch.Size([256])
model.vision_encoder.layer3.2.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.2.bn2.running_mean                    torch.Size([256])
model.vision_encoder.layer3.2.bn2.running_var                     torch.Size([256])
model.vision_encoder.layer3.2.bn2.weight                          torch.Size([256])
model.vision_encoder.layer3.2.bn3.bias                            torch.Size([1024])
model.vision_encoder.layer3.2.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.2.bn3.running_mean                    torch.Size([1024])
model.vision_encoder.layer3.2.bn3.running_var                     torch.Size([1024])
model.vision_encoder.layer3.2.bn3.weight                          torch.Size([1024])
model.vision_encoder.layer3.2.conv1.weight                        torch.Size([256, 1024, 1, 1])
model.vision_encoder.layer3.2.conv2.weight                        torch.Size([256, 256, 3, 3])
model.vision_encoder.layer3.2.conv3.weight                        torch.Size([1024, 256, 1, 1])

model.vision_encoder.layer3.3.bn1.bias                            torch.Size([256])
model.vision_encoder.layer3.3.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.3.bn1.running_mean                    torch.Size([256])
model.vision_encoder.layer3.3.bn1.running_var                     torch.Size([256])
model.vision_encoder.layer3.3.bn1.weight                          torch.Size([256])
model.vision_encoder.layer3.3.bn2.bias                            torch.Size([256])
model.vision_encoder.layer3.3.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.3.bn2.running_mean                    torch.Size([256])
model.vision_encoder.layer3.3.bn2.running_var                     torch.Size([256])
model.vision_encoder.layer3.3.bn2.weight                          torch.Size([256])
model.vision_encoder.layer3.3.bn3.bias                            torch.Size([1024])
model.vision_encoder.layer3.3.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.3.bn3.running_mean                    torch.Size([1024])
model.vision_encoder.layer3.3.bn3.running_var                     torch.Size([1024])
model.vision_encoder.layer3.3.bn3.weight                          torch.Size([1024])
model.vision_encoder.layer3.3.conv1.weight                        torch.Size([256, 1024, 1, 1])
model.vision_encoder.layer3.3.conv2.weight                        torch.Size([256, 256, 3, 3])
model.vision_encoder.layer3.3.conv3.weight                        torch.Size([1024, 256, 1, 1])

model.vision_encoder.layer3.4.bn1.bias                            torch.Size([256])
model.vision_encoder.layer3.4.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.4.bn1.running_mean                    torch.Size([256])
model.vision_encoder.layer3.4.bn1.running_var                     torch.Size([256])
model.vision_encoder.layer3.4.bn1.weight                          torch.Size([256])
model.vision_encoder.layer3.4.bn2.bias                            torch.Size([256])
model.vision_encoder.layer3.4.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.4.bn2.running_mean                    torch.Size([256])
model.vision_encoder.layer3.4.bn2.running_var                     torch.Size([256])
model.vision_encoder.layer3.4.bn2.weight                          torch.Size([256])
model.vision_encoder.layer3.4.bn3.bias                            torch.Size([1024])
model.vision_encoder.layer3.4.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.4.bn3.running_mean                    torch.Size([1024])
model.vision_encoder.layer3.4.bn3.running_var                     torch.Size([1024])
model.vision_encoder.layer3.4.bn3.weight                          torch.Size([1024])
model.vision_encoder.layer3.4.conv1.weight                        torch.Size([256, 1024, 1, 1])
model.vision_encoder.layer3.4.conv2.weight                        torch.Size([256, 256, 3, 3])
model.vision_encoder.layer3.4.conv3.weight                        torch.Size([1024, 256, 1, 1])

model.vision_encoder.layer3.5.bn1.bias                            torch.Size([256])
model.vision_encoder.layer3.5.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.5.bn1.running_mean                    torch.Size([256])
model.vision_encoder.layer3.5.bn1.running_var                     torch.Size([256])
model.vision_encoder.layer3.5.bn1.weight                          torch.Size([256])
model.vision_encoder.layer3.5.bn2.bias                            torch.Size([256])
model.vision_encoder.layer3.5.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.5.bn2.running_mean                    torch.Size([256])
model.vision_encoder.layer3.5.bn2.running_var                     torch.Size([256])
model.vision_encoder.layer3.5.bn2.weight                          torch.Size([256])
model.vision_encoder.layer3.5.bn3.bias                            torch.Size([1024])
model.vision_encoder.layer3.5.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer3.5.bn3.running_mean                    torch.Size([1024])
model.vision_encoder.layer3.5.bn3.running_var                     torch.Size([1024])
model.vision_encoder.layer3.5.bn3.weight                          torch.Size([1024])
model.vision_encoder.layer3.5.conv1.weight                        torch.Size([256, 1024, 1, 1])
model.vision_encoder.layer3.5.conv2.weight                        torch.Size([256, 256, 3, 3])
model.vision_encoder.layer3.5.conv3.weight                        torch.Size([1024, 256, 1, 1])


model.vision_encoder.layer4.0.bn1.bias                            torch.Size([512])
model.vision_encoder.layer4.0.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer4.0.bn1.running_mean                    torch.Size([512])
model.vision_encoder.layer4.0.bn1.running_var                     torch.Size([512])
model.vision_encoder.layer4.0.bn1.weight                          torch.Size([512])
model.vision_encoder.layer4.0.bn2.bias                            torch.Size([512])
model.vision_encoder.layer4.0.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer4.0.bn2.running_mean                    torch.Size([512])
model.vision_encoder.layer4.0.bn2.running_var                     torch.Size([512])
model.vision_encoder.layer4.0.bn2.weight                          torch.Size([512])
model.vision_encoder.layer4.0.bn3.bias                            torch.Size([2048])
model.vision_encoder.layer4.0.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer4.0.bn3.running_mean                    torch.Size([2048])
model.vision_encoder.layer4.0.bn3.running_var                     torch.Size([2048])
model.vision_encoder.layer4.0.bn3.weight                          torch.Size([2048])
model.vision_encoder.layer4.0.conv1.weight                        torch.Size([512, 1024, 1, 1])
model.vision_encoder.layer4.0.conv2.weight                        torch.Size([512, 512, 3, 3])
model.vision_encoder.layer4.0.conv3.weight                        torch.Size([2048, 512, 1, 1])
model.vision_encoder.layer4.0.downsample.0.weight                 torch.Size([2048, 1024, 1, 1])
model.vision_encoder.layer4.0.downsample.1.bias                   torch.Size([2048])
model.vision_encoder.layer4.0.downsample.1.num_batches_tracked    torch.Size([])
model.vision_encoder.layer4.0.downsample.1.running_mean           torch.Size([2048])
model.vision_encoder.layer4.0.downsample.1.running_var            torch.Size([2048])
model.vision_encoder.layer4.0.downsample.1.weight                 torch.Size([2048])

model.vision_encoder.layer4.1.bn1.bias                            torch.Size([512])
model.vision_encoder.layer4.1.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer4.1.bn1.running_mean                    torch.Size([512])
model.vision_encoder.layer4.1.bn1.running_var                     torch.Size([512])
model.vision_encoder.layer4.1.bn1.weight                          torch.Size([512])
model.vision_encoder.layer4.1.bn2.bias                            torch.Size([512])
model.vision_encoder.layer4.1.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer4.1.bn2.running_mean                    torch.Size([512])
model.vision_encoder.layer4.1.bn2.running_var                     torch.Size([512])
model.vision_encoder.layer4.1.bn2.weight                          torch.Size([512])
model.vision_encoder.layer4.1.bn3.bias                            torch.Size([2048])
model.vision_encoder.layer4.1.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer4.1.bn3.running_mean                    torch.Size([2048])
model.vision_encoder.layer4.1.bn3.running_var                     torch.Size([2048])
model.vision_encoder.layer4.1.bn3.weight                          torch.Size([2048])
model.vision_encoder.layer4.1.conv1.weight                        torch.Size([512, 2048, 1, 1])
model.vision_encoder.layer4.1.conv2.weight                        torch.Size([512, 512, 3, 3])
model.vision_encoder.layer4.1.conv3.weight                        torch.Size([2048, 512, 1, 1])

model.vision_encoder.layer4.2.bn1.bias                            torch.Size([512])
model.vision_encoder.layer4.2.bn1.num_batches_tracked             torch.Size([])
model.vision_encoder.layer4.2.bn1.running_mean                    torch.Size([512])
model.vision_encoder.layer4.2.bn1.running_var                     torch.Size([512])
model.vision_encoder.layer4.2.bn1.weight                          torch.Size([512])
model.vision_encoder.layer4.2.bn2.bias                            torch.Size([512])
model.vision_encoder.layer4.2.bn2.num_batches_tracked             torch.Size([])
model.vision_encoder.layer4.2.bn2.running_mean                    torch.Size([512])
model.vision_encoder.layer4.2.bn2.running_var                     torch.Size([512])
model.vision_encoder.layer4.2.bn2.weight                          torch.Size([512])
model.vision_encoder.layer4.2.bn3.bias                            torch.Size([2048])
model.vision_encoder.layer4.2.bn3.num_batches_tracked             torch.Size([])
model.vision_encoder.layer4.2.bn3.running_mean                    torch.Size([2048])
model.vision_encoder.layer4.2.bn3.running_var                     torch.Size([2048])
model.vision_encoder.layer4.2.bn3.weight                          torch.Size([2048])
model.vision_encoder.layer4.2.conv1.weight                        torch.Size([512, 2048, 1, 1])
model.vision_encoder.layer4.2.conv2.weight                        torch.Size([512, 512, 3, 3])
model.vision_encoder.layer4.2.conv3.weight                        torch.Size([2048, 512, 1, 1])
