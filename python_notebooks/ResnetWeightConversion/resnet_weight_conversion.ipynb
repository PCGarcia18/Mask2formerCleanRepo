{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path to weights = https://huggingface.co/BIFOLD-BigEarthNetv2-0/resnet50-s1-v0.2.0/blob/main/model.safetensors\n",
    "\n",
    "Its the version from Sentinel-1 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHA-256: 87990ba9058bc5413b90d7dc955ed1077cf8c54530db393801336209252b3739\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "BigEarthResnetPthPath = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/model_S1.safetensors'\n",
    "\n",
    "# Function to compute SHA-256 hash of a file\n",
    "def compute_sha256(file_path):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    \n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Read the file in chunks to handle large files efficiently\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    \n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "# Compute and print the SHA-256 hash of the file\n",
    "sha256_hash = compute_sha256(BigEarthResnetPthPath)\n",
    "print(f\"SHA-256: {sha256_hash}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Earth Resnet Weights\n",
      "Weight name                                                       Shape\n",
      "=====================================================================================\n",
      "model.vision_encoder.bn1.bias                                     torch.Size([64])\n",
      "model.vision_encoder.bn1.num_batches_tracked                      torch.Size([])\n",
      "model.vision_encoder.bn1.running_mean                             torch.Size([64])\n",
      "model.vision_encoder.bn1.running_var                              torch.Size([64])\n",
      "model.vision_encoder.bn1.weight                                   torch.Size([64])\n",
      "model.vision_encoder.conv1.weight                                 torch.Size([64, 2, 7, 7])\n",
      "model.vision_encoder.fc.bias                                      torch.Size([19])\n",
      "model.vision_encoder.fc.weight                                    torch.Size([19, 2048])\n",
      "model.vision_encoder.layer1.0.bn1.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.0.bn1.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn1.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn1.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn2.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.0.bn2.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn2.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn2.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn3.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer1.0.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.0.bn3.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer1.0.bn3.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer1.0.bn3.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer1.0.conv1.weight                        torch.Size([64, 64, 1, 1])\n",
      "model.vision_encoder.layer1.0.conv2.weight                        torch.Size([64, 64, 3, 3])\n",
      "model.vision_encoder.layer1.0.conv3.weight                        torch.Size([256, 64, 1, 1])\n",
      "model.vision_encoder.layer1.0.downsample.0.weight                 torch.Size([256, 64, 1, 1])\n",
      "model.vision_encoder.layer1.0.downsample.1.bias                   torch.Size([256])\n",
      "model.vision_encoder.layer1.0.downsample.1.num_batches_tracked    torch.Size([])\n",
      "model.vision_encoder.layer1.0.downsample.1.running_mean           torch.Size([256])\n",
      "model.vision_encoder.layer1.0.downsample.1.running_var            torch.Size([256])\n",
      "model.vision_encoder.layer1.0.downsample.1.weight                 torch.Size([256])\n",
      "model.vision_encoder.layer1.1.bn1.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.1.bn1.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn1.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn1.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn2.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.1.bn2.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn2.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn2.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn3.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer1.1.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.1.bn3.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer1.1.bn3.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer1.1.bn3.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer1.1.conv1.weight                        torch.Size([64, 256, 1, 1])\n",
      "model.vision_encoder.layer1.1.conv2.weight                        torch.Size([64, 64, 3, 3])\n",
      "model.vision_encoder.layer1.1.conv3.weight                        torch.Size([256, 64, 1, 1])\n",
      "model.vision_encoder.layer1.2.bn1.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.2.bn1.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn1.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn1.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn2.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.2.bn2.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn2.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn2.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn3.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer1.2.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.2.bn3.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer1.2.bn3.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer1.2.bn3.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer1.2.conv1.weight                        torch.Size([64, 256, 1, 1])\n",
      "model.vision_encoder.layer1.2.conv2.weight                        torch.Size([64, 64, 3, 3])\n",
      "model.vision_encoder.layer1.2.conv3.weight                        torch.Size([256, 64, 1, 1])\n",
      "model.vision_encoder.layer2.0.bn1.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.0.bn1.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn1.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn1.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn2.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.0.bn2.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn2.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn2.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn3.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer2.0.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.0.bn3.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer2.0.bn3.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer2.0.bn3.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer2.0.conv1.weight                        torch.Size([128, 256, 1, 1])\n",
      "model.vision_encoder.layer2.0.conv2.weight                        torch.Size([128, 128, 3, 3])\n",
      "model.vision_encoder.layer2.0.conv3.weight                        torch.Size([512, 128, 1, 1])\n",
      "model.vision_encoder.layer2.0.downsample.0.weight                 torch.Size([512, 256, 1, 1])\n",
      "model.vision_encoder.layer2.0.downsample.1.bias                   torch.Size([512])\n",
      "model.vision_encoder.layer2.0.downsample.1.num_batches_tracked    torch.Size([])\n",
      "model.vision_encoder.layer2.0.downsample.1.running_mean           torch.Size([512])\n",
      "model.vision_encoder.layer2.0.downsample.1.running_var            torch.Size([512])\n",
      "model.vision_encoder.layer2.0.downsample.1.weight                 torch.Size([512])\n",
      "model.vision_encoder.layer2.1.bn1.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.1.bn1.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn1.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn1.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn2.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.1.bn2.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn2.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn2.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn3.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer2.1.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.1.bn3.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer2.1.bn3.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer2.1.bn3.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer2.1.conv1.weight                        torch.Size([128, 512, 1, 1])\n",
      "model.vision_encoder.layer2.1.conv2.weight                        torch.Size([128, 128, 3, 3])\n",
      "model.vision_encoder.layer2.1.conv3.weight                        torch.Size([512, 128, 1, 1])\n",
      "model.vision_encoder.layer2.2.bn1.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.2.bn1.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn1.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn1.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn2.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.2.bn2.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn2.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn2.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn3.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer2.2.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.2.bn3.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer2.2.bn3.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer2.2.bn3.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer2.2.conv1.weight                        torch.Size([128, 512, 1, 1])\n",
      "model.vision_encoder.layer2.2.conv2.weight                        torch.Size([128, 128, 3, 3])\n",
      "model.vision_encoder.layer2.2.conv3.weight                        torch.Size([512, 128, 1, 1])\n",
      "model.vision_encoder.layer2.3.bn1.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.3.bn1.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn1.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn1.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn2.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.3.bn2.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn2.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn2.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn3.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer2.3.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.3.bn3.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer2.3.bn3.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer2.3.bn3.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer2.3.conv1.weight                        torch.Size([128, 512, 1, 1])\n",
      "model.vision_encoder.layer2.3.conv2.weight                        torch.Size([128, 128, 3, 3])\n",
      "model.vision_encoder.layer2.3.conv3.weight                        torch.Size([512, 128, 1, 1])\n",
      "model.vision_encoder.layer3.0.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.0.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.0.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.0.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.conv1.weight                        torch.Size([256, 512, 1, 1])\n",
      "model.vision_encoder.layer3.0.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.0.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer3.0.downsample.0.weight                 torch.Size([1024, 512, 1, 1])\n",
      "model.vision_encoder.layer3.0.downsample.1.bias                   torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.downsample.1.num_batches_tracked    torch.Size([])\n",
      "model.vision_encoder.layer3.0.downsample.1.running_mean           torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.downsample.1.running_var            torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.downsample.1.weight                 torch.Size([1024])\n",
      "model.vision_encoder.layer3.1.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.1.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.1.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.1.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.1.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.1.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.1.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.1.conv1.weight                        torch.Size([256, 1024, 1, 1])\n",
      "model.vision_encoder.layer3.1.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.1.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer3.2.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.2.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.2.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.2.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.2.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.2.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.2.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.2.conv1.weight                        torch.Size([256, 1024, 1, 1])\n",
      "model.vision_encoder.layer3.2.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.2.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer3.3.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.3.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.3.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.3.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.3.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.3.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.3.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.3.conv1.weight                        torch.Size([256, 1024, 1, 1])\n",
      "model.vision_encoder.layer3.3.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.3.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer3.4.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.4.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.4.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.4.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.4.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.4.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.4.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.4.conv1.weight                        torch.Size([256, 1024, 1, 1])\n",
      "model.vision_encoder.layer3.4.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.4.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer3.5.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.5.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.5.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.5.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.5.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.5.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.5.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.5.conv1.weight                        torch.Size([256, 1024, 1, 1])\n",
      "model.vision_encoder.layer3.5.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.5.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer4.0.bn1.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.0.bn1.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn1.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn1.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn2.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.0.bn2.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn2.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn2.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn3.bias                            torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.0.bn3.running_mean                    torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.bn3.running_var                     torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.bn3.weight                          torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.conv1.weight                        torch.Size([512, 1024, 1, 1])\n",
      "model.vision_encoder.layer4.0.conv2.weight                        torch.Size([512, 512, 3, 3])\n",
      "model.vision_encoder.layer4.0.conv3.weight                        torch.Size([2048, 512, 1, 1])\n",
      "model.vision_encoder.layer4.0.downsample.0.weight                 torch.Size([2048, 1024, 1, 1])\n",
      "model.vision_encoder.layer4.0.downsample.1.bias                   torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.downsample.1.num_batches_tracked    torch.Size([])\n",
      "model.vision_encoder.layer4.0.downsample.1.running_mean           torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.downsample.1.running_var            torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.downsample.1.weight                 torch.Size([2048])\n",
      "model.vision_encoder.layer4.1.bn1.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.1.bn1.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn1.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn1.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn2.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.1.bn2.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn2.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn2.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn3.bias                            torch.Size([2048])\n",
      "model.vision_encoder.layer4.1.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.1.bn3.running_mean                    torch.Size([2048])\n",
      "model.vision_encoder.layer4.1.bn3.running_var                     torch.Size([2048])\n",
      "model.vision_encoder.layer4.1.bn3.weight                          torch.Size([2048])\n",
      "model.vision_encoder.layer4.1.conv1.weight                        torch.Size([512, 2048, 1, 1])\n",
      "model.vision_encoder.layer4.1.conv2.weight                        torch.Size([512, 512, 3, 3])\n",
      "model.vision_encoder.layer4.1.conv3.weight                        torch.Size([2048, 512, 1, 1])\n",
      "model.vision_encoder.layer4.2.bn1.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.2.bn1.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn1.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn1.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn2.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.2.bn2.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn2.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn2.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn3.bias                            torch.Size([2048])\n",
      "model.vision_encoder.layer4.2.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.2.bn3.running_mean                    torch.Size([2048])\n",
      "model.vision_encoder.layer4.2.bn3.running_var                     torch.Size([2048])\n",
      "model.vision_encoder.layer4.2.bn3.weight                          torch.Size([2048])\n",
      "model.vision_encoder.layer4.2.conv1.weight                        torch.Size([512, 2048, 1, 1])\n",
      "model.vision_encoder.layer4.2.conv2.weight                        torch.Size([512, 512, 3, 3])\n",
      "model.vision_encoder.layer4.2.conv3.weight                        torch.Size([2048, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "name_width = 65\n",
    "with safe_open(BigEarthResnetPthPath, framework=\"pt\") as f:\n",
    "    # Print a header\n",
    "    print(\"Big Earth Resnet Weights\")\n",
    "    print(f\"{'Weight name'.ljust(name_width)} {'Shape'}\")\n",
    "    print(\"=\" * (name_width + 20))  # For a separator line\n",
    "\n",
    "    # Loop through each tensor\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"{key.ljust(name_width)} {str(tensor.shape)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHA-256: f55f8a5416d326c7e3139a97c6866e606dfc2e5cde30d52e2ccc47ce5f8b9be5\n",
      "Big Earth Resnet Weights Sentinel 2 (10 bands)\n",
      "Weight name                                                       Shape\n",
      "=====================================================================================\n",
      "model.vision_encoder.bn1.bias                                     torch.Size([64])\n",
      "model.vision_encoder.bn1.num_batches_tracked                      torch.Size([])\n",
      "model.vision_encoder.bn1.running_mean                             torch.Size([64])\n",
      "model.vision_encoder.bn1.running_var                              torch.Size([64])\n",
      "model.vision_encoder.bn1.weight                                   torch.Size([64])\n",
      "model.vision_encoder.conv1.weight                                 torch.Size([64, 10, 7, 7])\n",
      "model.vision_encoder.fc.bias                                      torch.Size([19])\n",
      "model.vision_encoder.fc.weight                                    torch.Size([19, 2048])\n",
      "model.vision_encoder.layer1.0.bn1.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.0.bn1.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn1.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn1.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn2.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.0.bn2.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn2.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn2.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.0.bn3.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer1.0.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.0.bn3.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer1.0.bn3.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer1.0.bn3.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer1.0.conv1.weight                        torch.Size([64, 64, 1, 1])\n",
      "model.vision_encoder.layer1.0.conv2.weight                        torch.Size([64, 64, 3, 3])\n",
      "model.vision_encoder.layer1.0.conv3.weight                        torch.Size([256, 64, 1, 1])\n",
      "model.vision_encoder.layer1.0.downsample.0.weight                 torch.Size([256, 64, 1, 1])\n",
      "model.vision_encoder.layer1.0.downsample.1.bias                   torch.Size([256])\n",
      "model.vision_encoder.layer1.0.downsample.1.num_batches_tracked    torch.Size([])\n",
      "model.vision_encoder.layer1.0.downsample.1.running_mean           torch.Size([256])\n",
      "model.vision_encoder.layer1.0.downsample.1.running_var            torch.Size([256])\n",
      "model.vision_encoder.layer1.0.downsample.1.weight                 torch.Size([256])\n",
      "model.vision_encoder.layer1.1.bn1.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.1.bn1.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn1.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn1.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn2.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.1.bn2.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn2.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn2.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.1.bn3.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer1.1.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.1.bn3.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer1.1.bn3.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer1.1.bn3.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer1.1.conv1.weight                        torch.Size([64, 256, 1, 1])\n",
      "model.vision_encoder.layer1.1.conv2.weight                        torch.Size([64, 64, 3, 3])\n",
      "model.vision_encoder.layer1.1.conv3.weight                        torch.Size([256, 64, 1, 1])\n",
      "model.vision_encoder.layer1.2.bn1.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.2.bn1.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn1.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn1.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn2.bias                            torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.2.bn2.running_mean                    torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn2.running_var                     torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn2.weight                          torch.Size([64])\n",
      "model.vision_encoder.layer1.2.bn3.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer1.2.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer1.2.bn3.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer1.2.bn3.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer1.2.bn3.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer1.2.conv1.weight                        torch.Size([64, 256, 1, 1])\n",
      "model.vision_encoder.layer1.2.conv2.weight                        torch.Size([64, 64, 3, 3])\n",
      "model.vision_encoder.layer1.2.conv3.weight                        torch.Size([256, 64, 1, 1])\n",
      "model.vision_encoder.layer2.0.bn1.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.0.bn1.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn1.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn1.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn2.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.0.bn2.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn2.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn2.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.0.bn3.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer2.0.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.0.bn3.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer2.0.bn3.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer2.0.bn3.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer2.0.conv1.weight                        torch.Size([128, 256, 1, 1])\n",
      "model.vision_encoder.layer2.0.conv2.weight                        torch.Size([128, 128, 3, 3])\n",
      "model.vision_encoder.layer2.0.conv3.weight                        torch.Size([512, 128, 1, 1])\n",
      "model.vision_encoder.layer2.0.downsample.0.weight                 torch.Size([512, 256, 1, 1])\n",
      "model.vision_encoder.layer2.0.downsample.1.bias                   torch.Size([512])\n",
      "model.vision_encoder.layer2.0.downsample.1.num_batches_tracked    torch.Size([])\n",
      "model.vision_encoder.layer2.0.downsample.1.running_mean           torch.Size([512])\n",
      "model.vision_encoder.layer2.0.downsample.1.running_var            torch.Size([512])\n",
      "model.vision_encoder.layer2.0.downsample.1.weight                 torch.Size([512])\n",
      "model.vision_encoder.layer2.1.bn1.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.1.bn1.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn1.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn1.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn2.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.1.bn2.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn2.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn2.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.1.bn3.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer2.1.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.1.bn3.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer2.1.bn3.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer2.1.bn3.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer2.1.conv1.weight                        torch.Size([128, 512, 1, 1])\n",
      "model.vision_encoder.layer2.1.conv2.weight                        torch.Size([128, 128, 3, 3])\n",
      "model.vision_encoder.layer2.1.conv3.weight                        torch.Size([512, 128, 1, 1])\n",
      "model.vision_encoder.layer2.2.bn1.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.2.bn1.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn1.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn1.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn2.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.2.bn2.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn2.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn2.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.2.bn3.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer2.2.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.2.bn3.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer2.2.bn3.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer2.2.bn3.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer2.2.conv1.weight                        torch.Size([128, 512, 1, 1])\n",
      "model.vision_encoder.layer2.2.conv2.weight                        torch.Size([128, 128, 3, 3])\n",
      "model.vision_encoder.layer2.2.conv3.weight                        torch.Size([512, 128, 1, 1])\n",
      "model.vision_encoder.layer2.3.bn1.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.3.bn1.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn1.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn1.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn2.bias                            torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.3.bn2.running_mean                    torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn2.running_var                     torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn2.weight                          torch.Size([128])\n",
      "model.vision_encoder.layer2.3.bn3.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer2.3.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer2.3.bn3.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer2.3.bn3.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer2.3.bn3.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer2.3.conv1.weight                        torch.Size([128, 512, 1, 1])\n",
      "model.vision_encoder.layer2.3.conv2.weight                        torch.Size([128, 128, 3, 3])\n",
      "model.vision_encoder.layer2.3.conv3.weight                        torch.Size([512, 128, 1, 1])\n",
      "model.vision_encoder.layer3.0.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.0.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.0.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.0.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.0.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.conv1.weight                        torch.Size([256, 512, 1, 1])\n",
      "model.vision_encoder.layer3.0.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.0.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer3.0.downsample.0.weight                 torch.Size([1024, 512, 1, 1])\n",
      "model.vision_encoder.layer3.0.downsample.1.bias                   torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.downsample.1.num_batches_tracked    torch.Size([])\n",
      "model.vision_encoder.layer3.0.downsample.1.running_mean           torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.downsample.1.running_var            torch.Size([1024])\n",
      "model.vision_encoder.layer3.0.downsample.1.weight                 torch.Size([1024])\n",
      "model.vision_encoder.layer3.1.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.1.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.1.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.1.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.1.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.1.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.1.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.1.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.1.conv1.weight                        torch.Size([256, 1024, 1, 1])\n",
      "model.vision_encoder.layer3.1.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.1.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer3.2.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.2.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.2.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.2.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.2.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.2.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.2.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.2.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.2.conv1.weight                        torch.Size([256, 1024, 1, 1])\n",
      "model.vision_encoder.layer3.2.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.2.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer3.3.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.3.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.3.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.3.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.3.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.3.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.3.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.3.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.3.conv1.weight                        torch.Size([256, 1024, 1, 1])\n",
      "model.vision_encoder.layer3.3.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.3.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer3.4.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.4.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.4.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.4.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.4.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.4.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.4.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.4.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.4.conv1.weight                        torch.Size([256, 1024, 1, 1])\n",
      "model.vision_encoder.layer3.4.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.4.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer3.5.bn1.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.5.bn1.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn1.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn1.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn2.bias                            torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.5.bn2.running_mean                    torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn2.running_var                     torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn2.weight                          torch.Size([256])\n",
      "model.vision_encoder.layer3.5.bn3.bias                            torch.Size([1024])\n",
      "model.vision_encoder.layer3.5.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer3.5.bn3.running_mean                    torch.Size([1024])\n",
      "model.vision_encoder.layer3.5.bn3.running_var                     torch.Size([1024])\n",
      "model.vision_encoder.layer3.5.bn3.weight                          torch.Size([1024])\n",
      "model.vision_encoder.layer3.5.conv1.weight                        torch.Size([256, 1024, 1, 1])\n",
      "model.vision_encoder.layer3.5.conv2.weight                        torch.Size([256, 256, 3, 3])\n",
      "model.vision_encoder.layer3.5.conv3.weight                        torch.Size([1024, 256, 1, 1])\n",
      "model.vision_encoder.layer4.0.bn1.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.0.bn1.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn1.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn1.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn2.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.0.bn2.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn2.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn2.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.0.bn3.bias                            torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.0.bn3.running_mean                    torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.bn3.running_var                     torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.bn3.weight                          torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.conv1.weight                        torch.Size([512, 1024, 1, 1])\n",
      "model.vision_encoder.layer4.0.conv2.weight                        torch.Size([512, 512, 3, 3])\n",
      "model.vision_encoder.layer4.0.conv3.weight                        torch.Size([2048, 512, 1, 1])\n",
      "model.vision_encoder.layer4.0.downsample.0.weight                 torch.Size([2048, 1024, 1, 1])\n",
      "model.vision_encoder.layer4.0.downsample.1.bias                   torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.downsample.1.num_batches_tracked    torch.Size([])\n",
      "model.vision_encoder.layer4.0.downsample.1.running_mean           torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.downsample.1.running_var            torch.Size([2048])\n",
      "model.vision_encoder.layer4.0.downsample.1.weight                 torch.Size([2048])\n",
      "model.vision_encoder.layer4.1.bn1.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.1.bn1.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn1.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn1.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn2.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.1.bn2.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn2.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn2.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.1.bn3.bias                            torch.Size([2048])\n",
      "model.vision_encoder.layer4.1.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.1.bn3.running_mean                    torch.Size([2048])\n",
      "model.vision_encoder.layer4.1.bn3.running_var                     torch.Size([2048])\n",
      "model.vision_encoder.layer4.1.bn3.weight                          torch.Size([2048])\n",
      "model.vision_encoder.layer4.1.conv1.weight                        torch.Size([512, 2048, 1, 1])\n",
      "model.vision_encoder.layer4.1.conv2.weight                        torch.Size([512, 512, 3, 3])\n",
      "model.vision_encoder.layer4.1.conv3.weight                        torch.Size([2048, 512, 1, 1])\n",
      "model.vision_encoder.layer4.2.bn1.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn1.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.2.bn1.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn1.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn1.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn2.bias                            torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn2.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.2.bn2.running_mean                    torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn2.running_var                     torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn2.weight                          torch.Size([512])\n",
      "model.vision_encoder.layer4.2.bn3.bias                            torch.Size([2048])\n",
      "model.vision_encoder.layer4.2.bn3.num_batches_tracked             torch.Size([])\n",
      "model.vision_encoder.layer4.2.bn3.running_mean                    torch.Size([2048])\n",
      "model.vision_encoder.layer4.2.bn3.running_var                     torch.Size([2048])\n",
      "model.vision_encoder.layer4.2.bn3.weight                          torch.Size([2048])\n",
      "model.vision_encoder.layer4.2.conv1.weight                        torch.Size([512, 2048, 1, 1])\n",
      "model.vision_encoder.layer4.2.conv2.weight                        torch.Size([512, 512, 3, 3])\n",
      "model.vision_encoder.layer4.2.conv3.weight                        torch.Size([2048, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# SENTINEL 2 Version\n",
    "BigEarthResnetPthPath = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/model_S2.safetensors'\n",
    "\n",
    "sha256_hash = compute_sha256(BigEarthResnetPthPath)\n",
    "print(f\"SHA-256: {sha256_hash}\")\n",
    "\n",
    "name_width = 65\n",
    "with safe_open(BigEarthResnetPthPath, framework=\"pt\") as f:\n",
    "    # Print a header\n",
    "    print(\"Big Earth Resnet Weights Sentinel 2 (10 bands)\")\n",
    "    print(f\"{'Weight name'.ljust(name_width)} {'Shape'}\")\n",
    "    print(\"=\" * (name_width + 20))  # For a separator line\n",
    "\n",
    "    # Loop through each tensor\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"{key.ljust(name_width)} {str(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask2Former Resnet Weights\n",
      "Weight name                                                       Shape\n",
      "=====================================================================================\n",
      "stem.conv1.weight                                                 (64, 3, 7, 7)\n",
      "stem.conv1.norm.running_mean                                      (64,)\n",
      "stem.conv1.norm.running_var                                       (64,)\n",
      "stem.conv1.norm.weight                                            (64,)\n",
      "stem.conv1.norm.bias                                              (64,)\n",
      "res2.0.conv1.weight                                               (64, 64, 1, 1)\n",
      "res2.0.conv1.norm.running_mean                                    (64,)\n",
      "res2.0.conv1.norm.running_var                                     (64,)\n",
      "res2.0.conv1.norm.weight                                          (64,)\n",
      "res2.0.conv1.norm.bias                                            (64,)\n",
      "res2.0.conv2.weight                                               (64, 64, 3, 3)\n",
      "res2.0.conv2.norm.running_mean                                    (64,)\n",
      "res2.0.conv2.norm.running_var                                     (64,)\n",
      "res2.0.conv2.norm.weight                                          (64,)\n",
      "res2.0.conv2.norm.bias                                            (64,)\n",
      "res2.0.conv3.weight                                               (256, 64, 1, 1)\n",
      "res2.0.conv3.norm.running_mean                                    (256,)\n",
      "res2.0.conv3.norm.running_var                                     (256,)\n",
      "res2.0.conv3.norm.weight                                          (256,)\n",
      "res2.0.conv3.norm.bias                                            (256,)\n",
      "res2.0.shortcut.weight                                            (256, 64, 1, 1)\n",
      "res2.0.shortcut.norm.running_mean                                 (256,)\n",
      "res2.0.shortcut.norm.running_var                                  (256,)\n",
      "res2.0.shortcut.norm.weight                                       (256,)\n",
      "res2.0.shortcut.norm.bias                                         (256,)\n",
      "res2.1.conv1.weight                                               (64, 256, 1, 1)\n",
      "res2.1.conv1.norm.running_mean                                    (64,)\n",
      "res2.1.conv1.norm.running_var                                     (64,)\n",
      "res2.1.conv1.norm.weight                                          (64,)\n",
      "res2.1.conv1.norm.bias                                            (64,)\n",
      "res2.1.conv2.weight                                               (64, 64, 3, 3)\n",
      "res2.1.conv2.norm.running_mean                                    (64,)\n",
      "res2.1.conv2.norm.running_var                                     (64,)\n",
      "res2.1.conv2.norm.weight                                          (64,)\n",
      "res2.1.conv2.norm.bias                                            (64,)\n",
      "res2.1.conv3.weight                                               (256, 64, 1, 1)\n",
      "res2.1.conv3.norm.running_mean                                    (256,)\n",
      "res2.1.conv3.norm.running_var                                     (256,)\n",
      "res2.1.conv3.norm.weight                                          (256,)\n",
      "res2.1.conv3.norm.bias                                            (256,)\n",
      "res2.2.conv1.weight                                               (64, 256, 1, 1)\n",
      "res2.2.conv1.norm.running_mean                                    (64,)\n",
      "res2.2.conv1.norm.running_var                                     (64,)\n",
      "res2.2.conv1.norm.weight                                          (64,)\n",
      "res2.2.conv1.norm.bias                                            (64,)\n",
      "res2.2.conv2.weight                                               (64, 64, 3, 3)\n",
      "res2.2.conv2.norm.running_mean                                    (64,)\n",
      "res2.2.conv2.norm.running_var                                     (64,)\n",
      "res2.2.conv2.norm.weight                                          (64,)\n",
      "res2.2.conv2.norm.bias                                            (64,)\n",
      "res2.2.conv3.weight                                               (256, 64, 1, 1)\n",
      "res2.2.conv3.norm.running_mean                                    (256,)\n",
      "res2.2.conv3.norm.running_var                                     (256,)\n",
      "res2.2.conv3.norm.weight                                          (256,)\n",
      "res2.2.conv3.norm.bias                                            (256,)\n",
      "res3.0.conv1.weight                                               (128, 256, 1, 1)\n",
      "res3.0.conv1.norm.running_mean                                    (128,)\n",
      "res3.0.conv1.norm.running_var                                     (128,)\n",
      "res3.0.conv1.norm.weight                                          (128,)\n",
      "res3.0.conv1.norm.bias                                            (128,)\n",
      "res3.0.conv2.weight                                               (128, 128, 3, 3)\n",
      "res3.0.conv2.norm.running_mean                                    (128,)\n",
      "res3.0.conv2.norm.running_var                                     (128,)\n",
      "res3.0.conv2.norm.weight                                          (128,)\n",
      "res3.0.conv2.norm.bias                                            (128,)\n",
      "res3.0.conv3.weight                                               (512, 128, 1, 1)\n",
      "res3.0.conv3.norm.running_mean                                    (512,)\n",
      "res3.0.conv3.norm.running_var                                     (512,)\n",
      "res3.0.conv3.norm.weight                                          (512,)\n",
      "res3.0.conv3.norm.bias                                            (512,)\n",
      "res3.0.shortcut.weight                                            (512, 256, 1, 1)\n",
      "res3.0.shortcut.norm.running_mean                                 (512,)\n",
      "res3.0.shortcut.norm.running_var                                  (512,)\n",
      "res3.0.shortcut.norm.weight                                       (512,)\n",
      "res3.0.shortcut.norm.bias                                         (512,)\n",
      "res3.1.conv1.weight                                               (128, 512, 1, 1)\n",
      "res3.1.conv1.norm.running_mean                                    (128,)\n",
      "res3.1.conv1.norm.running_var                                     (128,)\n",
      "res3.1.conv1.norm.weight                                          (128,)\n",
      "res3.1.conv1.norm.bias                                            (128,)\n",
      "res3.1.conv2.weight                                               (128, 128, 3, 3)\n",
      "res3.1.conv2.norm.running_mean                                    (128,)\n",
      "res3.1.conv2.norm.running_var                                     (128,)\n",
      "res3.1.conv2.norm.weight                                          (128,)\n",
      "res3.1.conv2.norm.bias                                            (128,)\n",
      "res3.1.conv3.weight                                               (512, 128, 1, 1)\n",
      "res3.1.conv3.norm.running_mean                                    (512,)\n",
      "res3.1.conv3.norm.running_var                                     (512,)\n",
      "res3.1.conv3.norm.weight                                          (512,)\n",
      "res3.1.conv3.norm.bias                                            (512,)\n",
      "res3.2.conv1.weight                                               (128, 512, 1, 1)\n",
      "res3.2.conv1.norm.running_mean                                    (128,)\n",
      "res3.2.conv1.norm.running_var                                     (128,)\n",
      "res3.2.conv1.norm.weight                                          (128,)\n",
      "res3.2.conv1.norm.bias                                            (128,)\n",
      "res3.2.conv2.weight                                               (128, 128, 3, 3)\n",
      "res3.2.conv2.norm.running_mean                                    (128,)\n",
      "res3.2.conv2.norm.running_var                                     (128,)\n",
      "res3.2.conv2.norm.weight                                          (128,)\n",
      "res3.2.conv2.norm.bias                                            (128,)\n",
      "res3.2.conv3.weight                                               (512, 128, 1, 1)\n",
      "res3.2.conv3.norm.running_mean                                    (512,)\n",
      "res3.2.conv3.norm.running_var                                     (512,)\n",
      "res3.2.conv3.norm.weight                                          (512,)\n",
      "res3.2.conv3.norm.bias                                            (512,)\n",
      "res3.3.conv1.weight                                               (128, 512, 1, 1)\n",
      "res3.3.conv1.norm.running_mean                                    (128,)\n",
      "res3.3.conv1.norm.running_var                                     (128,)\n",
      "res3.3.conv1.norm.weight                                          (128,)\n",
      "res3.3.conv1.norm.bias                                            (128,)\n",
      "res3.3.conv2.weight                                               (128, 128, 3, 3)\n",
      "res3.3.conv2.norm.running_mean                                    (128,)\n",
      "res3.3.conv2.norm.running_var                                     (128,)\n",
      "res3.3.conv2.norm.weight                                          (128,)\n",
      "res3.3.conv2.norm.bias                                            (128,)\n",
      "res3.3.conv3.weight                                               (512, 128, 1, 1)\n",
      "res3.3.conv3.norm.running_mean                                    (512,)\n",
      "res3.3.conv3.norm.running_var                                     (512,)\n",
      "res3.3.conv3.norm.weight                                          (512,)\n",
      "res3.3.conv3.norm.bias                                            (512,)\n",
      "res4.0.conv1.weight                                               (256, 512, 1, 1)\n",
      "res4.0.conv1.norm.running_mean                                    (256,)\n",
      "res4.0.conv1.norm.running_var                                     (256,)\n",
      "res4.0.conv1.norm.weight                                          (256,)\n",
      "res4.0.conv1.norm.bias                                            (256,)\n",
      "res4.0.conv2.weight                                               (256, 256, 3, 3)\n",
      "res4.0.conv2.norm.running_mean                                    (256,)\n",
      "res4.0.conv2.norm.running_var                                     (256,)\n",
      "res4.0.conv2.norm.weight                                          (256,)\n",
      "res4.0.conv2.norm.bias                                            (256,)\n",
      "res4.0.conv3.weight                                               (1024, 256, 1, 1)\n",
      "res4.0.conv3.norm.running_mean                                    (1024,)\n",
      "res4.0.conv3.norm.running_var                                     (1024,)\n",
      "res4.0.conv3.norm.weight                                          (1024,)\n",
      "res4.0.conv3.norm.bias                                            (1024,)\n",
      "res4.0.shortcut.weight                                            (1024, 512, 1, 1)\n",
      "res4.0.shortcut.norm.running_mean                                 (1024,)\n",
      "res4.0.shortcut.norm.running_var                                  (1024,)\n",
      "res4.0.shortcut.norm.weight                                       (1024,)\n",
      "res4.0.shortcut.norm.bias                                         (1024,)\n",
      "res4.1.conv1.weight                                               (256, 1024, 1, 1)\n",
      "res4.1.conv1.norm.running_mean                                    (256,)\n",
      "res4.1.conv1.norm.running_var                                     (256,)\n",
      "res4.1.conv1.norm.weight                                          (256,)\n",
      "res4.1.conv1.norm.bias                                            (256,)\n",
      "res4.1.conv2.weight                                               (256, 256, 3, 3)\n",
      "res4.1.conv2.norm.running_mean                                    (256,)\n",
      "res4.1.conv2.norm.running_var                                     (256,)\n",
      "res4.1.conv2.norm.weight                                          (256,)\n",
      "res4.1.conv2.norm.bias                                            (256,)\n",
      "res4.1.conv3.weight                                               (1024, 256, 1, 1)\n",
      "res4.1.conv3.norm.running_mean                                    (1024,)\n",
      "res4.1.conv3.norm.running_var                                     (1024,)\n",
      "res4.1.conv3.norm.weight                                          (1024,)\n",
      "res4.1.conv3.norm.bias                                            (1024,)\n",
      "res4.2.conv1.weight                                               (256, 1024, 1, 1)\n",
      "res4.2.conv1.norm.running_mean                                    (256,)\n",
      "res4.2.conv1.norm.running_var                                     (256,)\n",
      "res4.2.conv1.norm.weight                                          (256,)\n",
      "res4.2.conv1.norm.bias                                            (256,)\n",
      "res4.2.conv2.weight                                               (256, 256, 3, 3)\n",
      "res4.2.conv2.norm.running_mean                                    (256,)\n",
      "res4.2.conv2.norm.running_var                                     (256,)\n",
      "res4.2.conv2.norm.weight                                          (256,)\n",
      "res4.2.conv2.norm.bias                                            (256,)\n",
      "res4.2.conv3.weight                                               (1024, 256, 1, 1)\n",
      "res4.2.conv3.norm.running_mean                                    (1024,)\n",
      "res4.2.conv3.norm.running_var                                     (1024,)\n",
      "res4.2.conv3.norm.weight                                          (1024,)\n",
      "res4.2.conv3.norm.bias                                            (1024,)\n",
      "res4.3.conv1.weight                                               (256, 1024, 1, 1)\n",
      "res4.3.conv1.norm.running_mean                                    (256,)\n",
      "res4.3.conv1.norm.running_var                                     (256,)\n",
      "res4.3.conv1.norm.weight                                          (256,)\n",
      "res4.3.conv1.norm.bias                                            (256,)\n",
      "res4.3.conv2.weight                                               (256, 256, 3, 3)\n",
      "res4.3.conv2.norm.running_mean                                    (256,)\n",
      "res4.3.conv2.norm.running_var                                     (256,)\n",
      "res4.3.conv2.norm.weight                                          (256,)\n",
      "res4.3.conv2.norm.bias                                            (256,)\n",
      "res4.3.conv3.weight                                               (1024, 256, 1, 1)\n",
      "res4.3.conv3.norm.running_mean                                    (1024,)\n",
      "res4.3.conv3.norm.running_var                                     (1024,)\n",
      "res4.3.conv3.norm.weight                                          (1024,)\n",
      "res4.3.conv3.norm.bias                                            (1024,)\n",
      "res4.4.conv1.weight                                               (256, 1024, 1, 1)\n",
      "res4.4.conv1.norm.running_mean                                    (256,)\n",
      "res4.4.conv1.norm.running_var                                     (256,)\n",
      "res4.4.conv1.norm.weight                                          (256,)\n",
      "res4.4.conv1.norm.bias                                            (256,)\n",
      "res4.4.conv2.weight                                               (256, 256, 3, 3)\n",
      "res4.4.conv2.norm.running_mean                                    (256,)\n",
      "res4.4.conv2.norm.running_var                                     (256,)\n",
      "res4.4.conv2.norm.weight                                          (256,)\n",
      "res4.4.conv2.norm.bias                                            (256,)\n",
      "res4.4.conv3.weight                                               (1024, 256, 1, 1)\n",
      "res4.4.conv3.norm.running_mean                                    (1024,)\n",
      "res4.4.conv3.norm.running_var                                     (1024,)\n",
      "res4.4.conv3.norm.weight                                          (1024,)\n",
      "res4.4.conv3.norm.bias                                            (1024,)\n",
      "res4.5.conv1.weight                                               (256, 1024, 1, 1)\n",
      "res4.5.conv1.norm.running_mean                                    (256,)\n",
      "res4.5.conv1.norm.running_var                                     (256,)\n",
      "res4.5.conv1.norm.weight                                          (256,)\n",
      "res4.5.conv1.norm.bias                                            (256,)\n",
      "res4.5.conv2.weight                                               (256, 256, 3, 3)\n",
      "res4.5.conv2.norm.running_mean                                    (256,)\n",
      "res4.5.conv2.norm.running_var                                     (256,)\n",
      "res4.5.conv2.norm.weight                                          (256,)\n",
      "res4.5.conv2.norm.bias                                            (256,)\n",
      "res4.5.conv3.weight                                               (1024, 256, 1, 1)\n",
      "res4.5.conv3.norm.running_mean                                    (1024,)\n",
      "res4.5.conv3.norm.running_var                                     (1024,)\n",
      "res4.5.conv3.norm.weight                                          (1024,)\n",
      "res4.5.conv3.norm.bias                                            (1024,)\n",
      "res5.0.conv1.weight                                               (512, 1024, 1, 1)\n",
      "res5.0.conv1.norm.running_mean                                    (512,)\n",
      "res5.0.conv1.norm.running_var                                     (512,)\n",
      "res5.0.conv1.norm.weight                                          (512,)\n",
      "res5.0.conv1.norm.bias                                            (512,)\n",
      "res5.0.conv2.weight                                               (512, 512, 3, 3)\n",
      "res5.0.conv2.norm.running_mean                                    (512,)\n",
      "res5.0.conv2.norm.running_var                                     (512,)\n",
      "res5.0.conv2.norm.weight                                          (512,)\n",
      "res5.0.conv2.norm.bias                                            (512,)\n",
      "res5.0.conv3.weight                                               (2048, 512, 1, 1)\n",
      "res5.0.conv3.norm.running_mean                                    (2048,)\n",
      "res5.0.conv3.norm.running_var                                     (2048,)\n",
      "res5.0.conv3.norm.weight                                          (2048,)\n",
      "res5.0.conv3.norm.bias                                            (2048,)\n",
      "res5.0.shortcut.weight                                            (2048, 1024, 1, 1)\n",
      "res5.0.shortcut.norm.running_mean                                 (2048,)\n",
      "res5.0.shortcut.norm.running_var                                  (2048,)\n",
      "res5.0.shortcut.norm.weight                                       (2048,)\n",
      "res5.0.shortcut.norm.bias                                         (2048,)\n",
      "res5.1.conv1.weight                                               (512, 2048, 1, 1)\n",
      "res5.1.conv1.norm.running_mean                                    (512,)\n",
      "res5.1.conv1.norm.running_var                                     (512,)\n",
      "res5.1.conv1.norm.weight                                          (512,)\n",
      "res5.1.conv1.norm.bias                                            (512,)\n",
      "res5.1.conv2.weight                                               (512, 512, 3, 3)\n",
      "res5.1.conv2.norm.running_mean                                    (512,)\n",
      "res5.1.conv2.norm.running_var                                     (512,)\n",
      "res5.1.conv2.norm.weight                                          (512,)\n",
      "res5.1.conv2.norm.bias                                            (512,)\n",
      "res5.1.conv3.weight                                               (2048, 512, 1, 1)\n",
      "res5.1.conv3.norm.running_mean                                    (2048,)\n",
      "res5.1.conv3.norm.running_var                                     (2048,)\n",
      "res5.1.conv3.norm.weight                                          (2048,)\n",
      "res5.1.conv3.norm.bias                                            (2048,)\n",
      "res5.2.conv1.weight                                               (512, 2048, 1, 1)\n",
      "res5.2.conv1.norm.running_mean                                    (512,)\n",
      "res5.2.conv1.norm.running_var                                     (512,)\n",
      "res5.2.conv1.norm.weight                                          (512,)\n",
      "res5.2.conv1.norm.bias                                            (512,)\n",
      "res5.2.conv2.weight                                               (512, 512, 3, 3)\n",
      "res5.2.conv2.norm.running_mean                                    (512,)\n",
      "res5.2.conv2.norm.running_var                                     (512,)\n",
      "res5.2.conv2.norm.weight                                          (512,)\n",
      "res5.2.conv2.norm.bias                                            (512,)\n",
      "res5.2.conv3.weight                                               (2048, 512, 1, 1)\n",
      "res5.2.conv3.norm.running_mean                                    (2048,)\n",
      "res5.2.conv3.norm.running_var                                     (2048,)\n",
      "res5.2.conv3.norm.weight                                          (2048,)\n",
      "res5.2.conv3.norm.bias                                            (2048,)\n",
      "stem.fc.weight                                                    (1000, 2048)\n",
      "stem.fc.bias                                                      (1000,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "mask2formerResnetPklPath = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/R-50.pkl'\n",
    "\n",
    "# Load the weights from the pickle file\n",
    "with open(mask2formerResnetPklPath, 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "\n",
    "# Set the width for alignment\n",
    "name_width = 65  # Adjust this width based on the longest weight name\n",
    "\n",
    "# Print header\n",
    "print(\"Mask2Former Resnet Weights\")\n",
    "print(f\"{'Weight name'.ljust(name_width)} {'Shape'}\")\n",
    "print(\"=\" * (name_width + 20))  # Separator line\n",
    "\n",
    "# Loop through each weight and print its name and shape\n",
    "for key, tensor in weights[\"model\"].items():\n",
    "    print(f\"{key.ljust(name_width)} {str(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion from BigEarth to Mask2former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_from_Sentinel_to_Mask2former = {\n",
    "    \"model.vision_encoder.fc.weight\":\"stem.fc.weight \",\n",
    "    \"model.vision_encoder.fc.bias\":\"stem.fc.bias\",\n",
    "\n",
    "    #STEM\n",
    "    \"model.vision_encoder.conv1.weight\":\"stem.conv1.weight\",\n",
    "    \"model.vision_encoder.bn1.running_mean\":\"stem.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.bn1.running_var\":\"stem.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.bn1.weight\":\"stem.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.bn1.bias\":\"stem.conv1.norm.bias\",\n",
    "    \n",
    "    # RES2 Block Layer0\n",
    "    \"model.vision_encoder.layer1.0.conv1.weight\":\"res2.0.conv1.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn1.running_mean\":\"res2.0.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.0.bn1.running_var\":\"res2.0.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.0.bn1.weight\":\"res2.0.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn1.bias\":\"res2.0.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.0.conv2.weight\":\"res2.0.conv2.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn2.running_mean\":\"res2.0.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.0.bn2.running_var\":\"res2.0.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.0.bn2.weight\":\"res2.0.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn2.bias\":\"res2.0.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.0.conv3.weight\":\"res2.0.conv3.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn3.running_mean\":\"res2.0.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.0.bn3.running_var\":\"res2.0.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.0.bn3.weight\":\"res2.0.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn3.bias\":\"res2.0.conv3.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.0.downsample.0.weight\":\"res2.0.shortcut.weight\",\n",
    "    \"model.vision_encoder.layer1.0.downsample.1.running_mean\":\"res2.0.shortcut.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.0.downsample.1.running_var\":\"res2.0.shortcut.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.0.downsample.1.weight\":\"res2.0.shortcut.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.0.downsample.1.bias\":\"res2.0.shortcut.norm.bias\",\n",
    "\n",
    "    # RES2 Block Layer1\n",
    "    \"model.vision_encoder.layer1.1.conv1.weight\":\"res2.1.conv1.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn1.running_mean\":\"res2.1.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.1.bn1.running_var\":\"res2.1.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.1.bn1.weight\":\"res2.1.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn1.bias\":\"res2.1.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.1.conv2.weight\":\"res2.1.conv2.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn2.running_mean\":\"res2.1.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.1.bn2.running_var\":\"res2.1.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.1.bn2.weight\":\"res2.1.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn2.bias\":\"res2.1.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.1.conv3.weight\":\"res2.1.conv3.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn3.running_mean\":\"res2.1.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.1.bn3.running_var\":\"res2.1.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.1.bn3.weight\":\"res2.1.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn3.bias\":\"res2.1.conv3.norm.bias\",\n",
    "\n",
    "    # RES2 Block Layer2\n",
    "    \"model.vision_encoder.layer1.2.conv1.weight\":\"res2.2.conv1.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn1.running_mean\":\"res2.2.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.2.bn1.running_var\":\"res2.2.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.2.bn1.weight\":\"res2.2.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn1.bias\":\"res2.2.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.2.conv2.weight\":\"res2.2.conv2.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn2.running_mean\":\"res2.2.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.2.bn2.running_var\":\"res2.2.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.2.bn2.weight\":\"res2.2.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn2.bias\":\"res2.2.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.2.conv3.weight\":\"res2.2.conv3.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn3.running_mean\":\"res2.2.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.2.bn3.running_var\":\"res2.2.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.2.bn3.weight\":\"res2.2.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn3.bias\":\"res2.2.conv3.norm.bias\",\n",
    "\n",
    "    # RES3 Block Layer0\n",
    "    \"model.vision_encoder.layer2.0.conv1.weight\":\"res3.0.conv1.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn1.running_mean\":\"res3.0.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.0.bn1.running_var\":\"res3.0.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.0.bn1.weight\":\"res3.0.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn1.bias\":\"res3.0.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.0.conv2.weight\":\"res3.0.conv2.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn2.running_mean\":\"res3.0.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.0.bn2.running_var\":\"res3.0.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.0.bn2.weight\":\"res3.0.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn2.bias\":\"res3.0.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.0.conv3.weight\":\"res3.0.conv3.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn3.running_mean\":\"res3.0.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.0.bn3.running_var\":\"res3.0.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.0.bn3.weight\":\"res3.0.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn3.bias\":\"res3.0.conv3.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.0.downsample.0.weight\":\"res3.0.shortcut.weight\",\n",
    "    \"model.vision_encoder.layer2.0.downsample.1.running_mean\":\"res3.0.shortcut.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.0.downsample.1.running_var\":\"res3.0.shortcut.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.0.downsample.1.weight\":\"res3.0.shortcut.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.0.downsample.1.bias\":\"res3.0.shortcut.norm.bias\",\n",
    "\n",
    "    # RES3 Block Layer1\n",
    "    \"model.vision_encoder.layer2.1.conv1.weight\":\"res3.1.conv1.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn1.running_mean\":\"res3.1.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.1.bn1.running_var\":\"res3.1.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.1.bn1.weight\":\"res3.1.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn1.bias\":\"res3.1.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.1.conv2.weight\":\"res3.1.conv2.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn2.running_mean\":\"res3.1.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.1.bn2.running_var\":\"res3.1.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.1.bn2.weight\":\"res3.1.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn2.bias\":\"res3.1.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.1.conv3.weight\":\"res3.1.conv3.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn3.running_mean\":\"res3.1.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.1.bn3.running_var\":\"res3.1.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.1.bn3.weight\":\"res3.1.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn3.bias\":\"res3.1.conv3.norm.bias\",\n",
    "\n",
    "    # RES3 Block Layer2\n",
    "    \"model.vision_encoder.layer2.2.conv1.weight\":\"res3.2.conv1.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn1.running_mean\":\"res3.2.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.2.bn1.running_var\":\"res3.2.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.2.bn1.weight\":\"res3.2.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn1.bias\":\"res3.2.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.2.conv2.weight\":\"res3.2.conv2.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn2.running_mean\":\"res3.2.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.2.bn2.running_var\":\"res3.2.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.2.bn2.weight\":\"res3.2.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn2.bias\":\"res3.2.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.2.conv3.weight\":\"res3.2.conv3.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn3.running_mean\":\"res3.2.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.2.bn3.running_var\":\"res3.2.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.2.bn3.weight\":\"res3.2.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn3.bias\":\"res3.2.conv3.norm.bias\",\n",
    "\n",
    "    # RES3 Block Layer3\n",
    "    \"model.vision_encoder.layer2.3.conv1.weight\":\"res3.3.conv1.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn1.running_mean\":\"res3.3.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.3.bn1.running_var\":\"res3.3.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.3.bn1.weight\":\"res3.3.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn1.bias\":\"res3.3.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.3.conv2.weight\":\"res3.3.conv2.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn2.running_mean\":\"res3.3.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.3.bn2.running_var\":\"res3.3.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.3.bn2.weight\":\"res3.3.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn2.bias\":\"res3.3.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.3.conv3.weight\":\"res3.3.conv3.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn3.running_mean\":\"res3.3.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.3.bn3.running_var\":\"res3.3.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.3.bn3.weight\":\"res3.3.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn3.bias\":\"res3.3.conv3.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer0\n",
    "    \"model.vision_encoder.layer3.0.conv1.weight\":\"res4.0.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn1.running_mean\":\"res4.0.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.0.bn1.running_var\":\"res4.0.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.0.bn1.weight\":\"res4.0.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn1.bias\":\"res4.0.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.0.conv2.weight\":\"res4.0.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn2.running_mean\":\"res4.0.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.0.bn2.running_var\":\"res4.0.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.0.bn2.weight\":\"res4.0.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn2.bias\":\"res4.0.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.0.conv3.weight\":\"res4.0.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn3.running_mean\":\"res4.0.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.0.bn3.running_var\":\"res4.0.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.0.bn3.weight\":\"res4.0.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn3.bias\":\"res4.0.conv3.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.0.downsample.0.weight\":\"res4.0.shortcut.weight\",\n",
    "    \"model.vision_encoder.layer3.0.downsample.1.running_mean\":\"res4.0.shortcut.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.0.downsample.1.running_var\":\"res4.0.shortcut.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.0.downsample.1.weight\":\"res4.0.shortcut.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.0.downsample.1.bias\":\"res4.0.shortcut.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer1\n",
    "    \"model.vision_encoder.layer3.1.conv1.weight\":\"res4.1.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn1.running_mean\":\"res4.1.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.1.bn1.running_var\":\"res4.1.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.1.bn1.weight\":\"res4.1.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn1.bias\":\"res4.1.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.1.conv2.weight\":\"res4.1.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn2.running_mean\":\"res4.1.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.1.bn2.running_var\":\"res4.1.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.1.bn2.weight\":\"res4.1.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn2.bias\":\"res4.1.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.1.conv3.weight\":\"res4.1.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn3.running_mean\":\"res4.1.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.1.bn3.running_var\":\"res4.1.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.1.bn3.weight\":\"res4.1.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn3.bias\":\"res4.1.conv3.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer2\n",
    "    \"model.vision_encoder.layer3.2.conv1.weight\":\"res4.2.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn1.running_mean\":\"res4.2.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.2.bn1.running_var\":\"res4.2.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.2.bn1.weight\":\"res4.2.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn1.bias\":\"res4.2.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.2.conv2.weight\":\"res4.2.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn2.running_mean\":\"res4.2.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.2.bn2.running_var\":\"res4.2.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.2.bn2.weight\":\"res4.2.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn2.bias\":\"res4.2.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.2.conv3.weight\":\"res4.2.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn3.running_mean\":\"res4.2.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.2.bn3.running_var\":\"res4.2.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.2.bn3.weight\":\"res4.2.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn3.bias\":\"res4.2.conv3.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer3\n",
    "    \"model.vision_encoder.layer3.3.conv1.weight\":\"res4.3.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn1.running_mean\":\"res4.3.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.3.bn1.running_var\":\"res4.3.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.3.bn1.weight\":\"res4.3.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn1.bias\":\"res4.3.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.3.conv2.weight\":\"res4.3.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn2.running_mean\":\"res4.3.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.3.bn2.running_var\":\"res4.3.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.3.bn2.weight\":\"res4.3.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn2.bias\":\"res4.3.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.3.conv3.weight\":\"res4.3.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn3.running_mean\":\"res4.3.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.3.bn3.running_var\":\"res4.3.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.3.bn3.weight\":\"res4.3.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn3.bias\":\"res4.3.conv3.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer4\n",
    "    \"model.vision_encoder.layer3.4.conv1.weight\":\"res4.4.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn1.running_mean\":\"res4.4.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.4.bn1.running_var\":\"res4.4.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.4.bn1.weight\":\"res4.4.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn1.bias\":\"res4.4.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.4.conv2.weight\":\"res4.4.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn2.running_mean\":\"res4.4.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.4.bn2.running_var\":\"res4.4.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.4.bn2.weight\":\"res4.4.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn2.bias\":\"res4.4.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.4.conv3.weight\":\"res4.4.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn3.running_mean\":\"res4.4.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.4.bn3.running_var\":\"res4.4.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.4.bn3.weight\":\"res4.4.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn3.bias\":\"res4.4.conv3.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer5\n",
    "    \"model.vision_encoder.layer3.5.conv1.weight\":\"res4.5.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn1.running_mean\":\"res4.5.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.5.bn1.running_var\":\"res4.5.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.5.bn1.weight\":\"res4.5.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn1.bias\":\"res4.5.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.5.conv2.weight\":\"res4.5.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn2.running_mean\":\"res4.5.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.5.bn2.running_var\":\"res4.5.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.5.bn2.weight\":\"res4.5.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn2.bias\":\"res4.5.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.5.conv3.weight\":\"res4.5.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn3.running_mean\":\"res4.5.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.5.bn3.running_var\":\"res4.5.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.5.bn3.weight\":\"res4.5.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn3.bias\":\"res4.5.conv3.norm.bias\",\n",
    "\n",
    "    # RES5 Block Layer0\n",
    "    \"model.vision_encoder.layer4.0.conv1.weight\":\"res5.0.conv1.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn1.running_mean\":\"res5.0.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.0.bn1.running_var\":\"res5.0.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.0.bn1.weight\":\"res5.0.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn1.bias\":\"res5.0.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.0.conv2.weight\":\"res5.0.conv2.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn2.running_mean\":\"res5.0.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.0.bn2.running_var\":\"res5.0.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.0.bn2.weight\":\"res5.0.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn2.bias\":\"res5.0.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.0.conv3.weight\":\"res5.0.conv3.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn3.running_mean\":\"res5.0.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.0.bn3.running_var\":\"res5.0.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.0.bn3.weight\":\"res5.0.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn3.bias\":\"res5.0.conv3.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.0.downsample.0.weight\":\"res5.0.shortcut.weight\",\n",
    "    \"model.vision_encoder.layer4.0.downsample.1.running_mean\":\"res5.0.shortcut.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.0.downsample.1.running_var\":\"res5.0.shortcut.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.0.downsample.1.weight\":\"res5.0.shortcut.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.0.downsample.1.bias\":\"res5.0.shortcut.norm.bias\",\n",
    "\n",
    "    # RES5 Block Layer1\n",
    "    \"model.vision_encoder.layer4.1.conv1.weight\":\"res5.1.conv1.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn1.running_mean\":\"res5.1.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.1.bn1.running_var\":\"res5.1.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.1.bn1.weight\":\"res5.1.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn1.bias\":\"res5.1.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.1.conv2.weight\":\"res5.1.conv2.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn2.running_mean\":\"res5.1.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.1.bn2.running_var\":\"res5.1.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.1.bn2.weight\":\"res5.1.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn2.bias\":\"res5.1.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.1.conv3.weight\":\"res5.1.conv3.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn3.running_mean\":\"res5.1.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.1.bn3.running_var\":\"res5.1.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.1.bn3.weight\":\"res5.1.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn3.bias\":\"res5.1.conv3.norm.bias\",\n",
    "\n",
    "    # RES5 Block Layer2\n",
    "    \"model.vision_encoder.layer4.2.conv1.weight\":\"res5.2.conv1.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn1.running_mean\":\"res5.2.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.2.bn1.running_var\":\"res5.2.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.2.bn1.weight\":\"res5.2.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn1.bias\":\"res5.2.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.2.conv2.weight\":\"res5.2.conv2.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn2.running_mean\":\"res5.2.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.2.bn2.running_var\":\"res5.2.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.2.bn2.weight\":\"res5.2.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn2.bias\":\"res5.2.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.2.conv3.weight\":\"res5.2.conv3.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn3.running_mean\":\"res5.2.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.2.bn3.running_var\":\"res5.2.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.2.bn3.weight\":\"res5.2.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn3.bias\":\"res5.2.conv3.norm.bias\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem.conv1.norm.bias\n",
      "(64,)\n",
      "stem.conv1.norm.running_mean\n",
      "(64,)\n",
      "stem.conv1.norm.running_var\n",
      "(64,)\n",
      "stem.conv1.norm.weight\n",
      "(64,)\n",
      "stem.conv1.weight\n",
      "(64, 10, 7, 7)\n",
      "stem.fc.bias\n",
      "(19,)\n",
      "stem.fc.weight \n",
      "(19, 2048)\n",
      "res2.0.conv1.norm.bias\n",
      "(64,)\n",
      "res2.0.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.0.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.0.conv1.norm.weight\n",
      "(64,)\n",
      "res2.0.conv2.norm.bias\n",
      "(64,)\n",
      "res2.0.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.0.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.0.conv2.norm.weight\n",
      "(64,)\n",
      "res2.0.conv3.norm.bias\n",
      "(256,)\n",
      "res2.0.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.0.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.0.conv3.norm.weight\n",
      "(256,)\n",
      "res2.0.conv1.weight\n",
      "(64, 64, 1, 1)\n",
      "res2.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.0.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.0.shortcut.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.0.shortcut.norm.bias\n",
      "(256,)\n",
      "res2.0.shortcut.norm.running_mean\n",
      "(256,)\n",
      "res2.0.shortcut.norm.running_var\n",
      "(256,)\n",
      "res2.0.shortcut.norm.weight\n",
      "(256,)\n",
      "res2.1.conv1.norm.bias\n",
      "(64,)\n",
      "res2.1.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.1.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.1.conv1.norm.weight\n",
      "(64,)\n",
      "res2.1.conv2.norm.bias\n",
      "(64,)\n",
      "res2.1.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.1.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.1.conv2.norm.weight\n",
      "(64,)\n",
      "res2.1.conv3.norm.bias\n",
      "(256,)\n",
      "res2.1.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.1.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.1.conv3.norm.weight\n",
      "(256,)\n",
      "res2.1.conv1.weight\n",
      "(64, 256, 1, 1)\n",
      "res2.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.1.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.2.conv1.norm.bias\n",
      "(64,)\n",
      "res2.2.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.2.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.2.conv1.norm.weight\n",
      "(64,)\n",
      "res2.2.conv2.norm.bias\n",
      "(64,)\n",
      "res2.2.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.2.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.2.conv2.norm.weight\n",
      "(64,)\n",
      "res2.2.conv3.norm.bias\n",
      "(256,)\n",
      "res2.2.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.2.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.2.conv3.norm.weight\n",
      "(256,)\n",
      "res2.2.conv1.weight\n",
      "(64, 256, 1, 1)\n",
      "res2.2.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.2.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res3.0.conv1.norm.bias\n",
      "(128,)\n",
      "res3.0.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.0.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.0.conv1.norm.weight\n",
      "(128,)\n",
      "res3.0.conv2.norm.bias\n",
      "(128,)\n",
      "res3.0.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.0.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.0.conv2.norm.weight\n",
      "(128,)\n",
      "res3.0.conv3.norm.bias\n",
      "(512,)\n",
      "res3.0.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.0.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.0.conv3.norm.weight\n",
      "(512,)\n",
      "res3.0.conv1.weight\n",
      "(128, 256, 1, 1)\n",
      "res3.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.0.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.0.shortcut.weight\n",
      "(512, 256, 1, 1)\n",
      "res3.0.shortcut.norm.bias\n",
      "(512,)\n",
      "res3.0.shortcut.norm.running_mean\n",
      "(512,)\n",
      "res3.0.shortcut.norm.running_var\n",
      "(512,)\n",
      "res3.0.shortcut.norm.weight\n",
      "(512,)\n",
      "res3.1.conv1.norm.bias\n",
      "(128,)\n",
      "res3.1.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.1.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.1.conv1.norm.weight\n",
      "(128,)\n",
      "res3.1.conv2.norm.bias\n",
      "(128,)\n",
      "res3.1.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.1.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.1.conv2.norm.weight\n",
      "(128,)\n",
      "res3.1.conv3.norm.bias\n",
      "(512,)\n",
      "res3.1.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.1.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.1.conv3.norm.weight\n",
      "(512,)\n",
      "res3.1.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.1.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.2.conv1.norm.bias\n",
      "(128,)\n",
      "res3.2.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.2.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.2.conv1.norm.weight\n",
      "(128,)\n",
      "res3.2.conv2.norm.bias\n",
      "(128,)\n",
      "res3.2.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.2.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.2.conv2.norm.weight\n",
      "(128,)\n",
      "res3.2.conv3.norm.bias\n",
      "(512,)\n",
      "res3.2.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.2.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.2.conv3.norm.weight\n",
      "(512,)\n",
      "res3.2.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.2.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.2.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.3.conv1.norm.bias\n",
      "(128,)\n",
      "res3.3.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.3.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.3.conv1.norm.weight\n",
      "(128,)\n",
      "res3.3.conv2.norm.bias\n",
      "(128,)\n",
      "res3.3.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.3.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.3.conv2.norm.weight\n",
      "(128,)\n",
      "res3.3.conv3.norm.bias\n",
      "(512,)\n",
      "res3.3.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.3.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.3.conv3.norm.weight\n",
      "(512,)\n",
      "res3.3.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.3.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.3.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res4.0.conv1.norm.bias\n",
      "(256,)\n",
      "res4.0.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.0.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.0.conv1.norm.weight\n",
      "(256,)\n",
      "res4.0.conv2.norm.bias\n",
      "(256,)\n",
      "res4.0.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.0.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.0.conv2.norm.weight\n",
      "(256,)\n",
      "res4.0.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.0.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.0.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.0.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.0.conv1.weight\n",
      "(256, 512, 1, 1)\n",
      "res4.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.0.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.0.shortcut.weight\n",
      "(1024, 512, 1, 1)\n",
      "res4.0.shortcut.norm.bias\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.running_mean\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.running_var\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.weight\n",
      "(1024,)\n",
      "res4.1.conv1.norm.bias\n",
      "(256,)\n",
      "res4.1.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.1.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.1.conv1.norm.weight\n",
      "(256,)\n",
      "res4.1.conv2.norm.bias\n",
      "(256,)\n",
      "res4.1.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.1.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.1.conv2.norm.weight\n",
      "(256,)\n",
      "res4.1.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.1.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.1.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.1.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.1.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.1.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.2.conv1.norm.bias\n",
      "(256,)\n",
      "res4.2.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.2.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.2.conv1.norm.weight\n",
      "(256,)\n",
      "res4.2.conv2.norm.bias\n",
      "(256,)\n",
      "res4.2.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.2.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.2.conv2.norm.weight\n",
      "(256,)\n",
      "res4.2.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.2.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.2.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.2.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.2.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.2.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.2.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.3.conv1.norm.bias\n",
      "(256,)\n",
      "res4.3.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.3.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.3.conv1.norm.weight\n",
      "(256,)\n",
      "res4.3.conv2.norm.bias\n",
      "(256,)\n",
      "res4.3.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.3.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.3.conv2.norm.weight\n",
      "(256,)\n",
      "res4.3.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.3.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.3.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.3.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.3.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.3.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.3.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.4.conv1.norm.bias\n",
      "(256,)\n",
      "res4.4.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.4.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.4.conv1.norm.weight\n",
      "(256,)\n",
      "res4.4.conv2.norm.bias\n",
      "(256,)\n",
      "res4.4.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.4.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.4.conv2.norm.weight\n",
      "(256,)\n",
      "res4.4.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.4.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.4.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.4.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.4.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.4.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.4.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.5.conv1.norm.bias\n",
      "(256,)\n",
      "res4.5.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.5.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.5.conv1.norm.weight\n",
      "(256,)\n",
      "res4.5.conv2.norm.bias\n",
      "(256,)\n",
      "res4.5.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.5.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.5.conv2.norm.weight\n",
      "(256,)\n",
      "res4.5.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.5.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.5.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.5.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.5.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.5.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.5.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res5.0.conv1.norm.bias\n",
      "(512,)\n",
      "res5.0.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.0.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.0.conv1.norm.weight\n",
      "(512,)\n",
      "res5.0.conv2.norm.bias\n",
      "(512,)\n",
      "res5.0.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.0.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.0.conv2.norm.weight\n",
      "(512,)\n",
      "res5.0.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.0.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.0.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.0.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.0.conv1.weight\n",
      "(512, 1024, 1, 1)\n",
      "res5.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.0.conv3.weight\n",
      "(2048, 512, 1, 1)\n",
      "res5.0.shortcut.weight\n",
      "(2048, 1024, 1, 1)\n",
      "res5.0.shortcut.norm.bias\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.running_mean\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.running_var\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.weight\n",
      "(2048,)\n",
      "res5.1.conv1.norm.bias\n",
      "(512,)\n",
      "res5.1.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.1.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.1.conv1.norm.weight\n",
      "(512,)\n",
      "res5.1.conv2.norm.bias\n",
      "(512,)\n",
      "res5.1.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.1.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.1.conv2.norm.weight\n",
      "(512,)\n",
      "res5.1.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.1.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.1.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.1.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.1.conv1.weight\n",
      "(512, 2048, 1, 1)\n",
      "res5.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.1.conv3.weight\n",
      "(2048, 512, 1, 1)\n",
      "res5.2.conv1.norm.bias\n",
      "(512,)\n",
      "res5.2.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.2.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.2.conv1.norm.weight\n",
      "(512,)\n",
      "res5.2.conv2.norm.bias\n",
      "(512,)\n",
      "res5.2.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.2.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.2.conv2.norm.weight\n",
      "(512,)\n",
      "res5.2.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.2.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.2.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.2.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.2.conv1.weight\n",
      "(512, 2048, 1, 1)\n",
      "res5.2.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.2.conv3.weight\n",
      "(2048, 512, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mask2formerResnetPklPath = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/R-50.pkl'\n",
    "BigEarthResnetPthPath = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/model_S2.safetensors'\n",
    "\n",
    "# Keep the Bigearth weights in a dictionary with the keys from the Mask2Former model\n",
    "bigearth_weights = {}\n",
    "with safe_open(BigEarthResnetPthPath, framework=\"pt\") as f:\n",
    "    for key in f.keys():\n",
    "        if key in dictionary_from_Sentinel_to_Mask2former:\n",
    "            new_key = dictionary_from_Sentinel_to_Mask2former[key]\n",
    "            bigearth_weights[new_key] = np.array(f.get_tensor(key))\n",
    "\n",
    "for key in bigearth_weights.keys():\n",
    "    print(key)\n",
    "    print(bigearth_weights[key].shape)\n",
    "\n",
    "# Load the weights from the pickle file\n",
    "with open(mask2formerResnetPklPath, 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "\n",
    "weights[\"model\"] = bigearth_weights\n",
    "\n",
    "# Save the updated weights to a new pickle file\n",
    "new_pkl_path = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/R-50_S2.pkl'\n",
    "with open(new_pkl_path, 'wb') as f:\n",
    "    pickle.dump(weights, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', '__author__', 'matching_heuristics'])\n",
      "dict_keys(['stem.conv1.norm.bias', 'stem.conv1.norm.running_mean', 'stem.conv1.norm.running_var', 'stem.conv1.norm.weight', 'stem.conv1.weight', 'stem.fc.bias', 'stem.fc.weight ', 'res2.0.conv1.norm.bias', 'res2.0.conv1.norm.running_mean', 'res2.0.conv1.norm.running_var', 'res2.0.conv1.norm.weight', 'res2.0.conv2.norm.bias', 'res2.0.conv2.norm.running_mean', 'res2.0.conv2.norm.running_var', 'res2.0.conv2.norm.weight', 'res2.0.conv3.norm.bias', 'res2.0.conv3.norm.running_mean', 'res2.0.conv3.norm.running_var', 'res2.0.conv3.norm.weight', 'res2.0.conv1.weight', 'res2.0.conv2.weight', 'res2.0.conv3.weight', 'res2.0.shortcut.weight', 'res2.0.shortcut.norm.bias', 'res2.0.shortcut.norm.running_mean', 'res2.0.shortcut.norm.running_var', 'res2.0.shortcut.norm.weight', 'res2.1.conv1.norm.bias', 'res2.1.conv1.norm.running_mean', 'res2.1.conv1.norm.running_var', 'res2.1.conv1.norm.weight', 'res2.1.conv2.norm.bias', 'res2.1.conv2.norm.running_mean', 'res2.1.conv2.norm.running_var', 'res2.1.conv2.norm.weight', 'res2.1.conv3.norm.bias', 'res2.1.conv3.norm.running_mean', 'res2.1.conv3.norm.running_var', 'res2.1.conv3.norm.weight', 'res2.1.conv1.weight', 'res2.1.conv2.weight', 'res2.1.conv3.weight', 'res2.2.conv1.norm.bias', 'res2.2.conv1.norm.running_mean', 'res2.2.conv1.norm.running_var', 'res2.2.conv1.norm.weight', 'res2.2.conv2.norm.bias', 'res2.2.conv2.norm.running_mean', 'res2.2.conv2.norm.running_var', 'res2.2.conv2.norm.weight', 'res2.2.conv3.norm.bias', 'res2.2.conv3.norm.running_mean', 'res2.2.conv3.norm.running_var', 'res2.2.conv3.norm.weight', 'res2.2.conv1.weight', 'res2.2.conv2.weight', 'res2.2.conv3.weight', 'res3.0.conv1.norm.bias', 'res3.0.conv1.norm.running_mean', 'res3.0.conv1.norm.running_var', 'res3.0.conv1.norm.weight', 'res3.0.conv2.norm.bias', 'res3.0.conv2.norm.running_mean', 'res3.0.conv2.norm.running_var', 'res3.0.conv2.norm.weight', 'res3.0.conv3.norm.bias', 'res3.0.conv3.norm.running_mean', 'res3.0.conv3.norm.running_var', 'res3.0.conv3.norm.weight', 'res3.0.conv1.weight', 'res3.0.conv2.weight', 'res3.0.conv3.weight', 'res3.0.shortcut.weight', 'res3.0.shortcut.norm.bias', 'res3.0.shortcut.norm.running_mean', 'res3.0.shortcut.norm.running_var', 'res3.0.shortcut.norm.weight', 'res3.1.conv1.norm.bias', 'res3.1.conv1.norm.running_mean', 'res3.1.conv1.norm.running_var', 'res3.1.conv1.norm.weight', 'res3.1.conv2.norm.bias', 'res3.1.conv2.norm.running_mean', 'res3.1.conv2.norm.running_var', 'res3.1.conv2.norm.weight', 'res3.1.conv3.norm.bias', 'res3.1.conv3.norm.running_mean', 'res3.1.conv3.norm.running_var', 'res3.1.conv3.norm.weight', 'res3.1.conv1.weight', 'res3.1.conv2.weight', 'res3.1.conv3.weight', 'res3.2.conv1.norm.bias', 'res3.2.conv1.norm.running_mean', 'res3.2.conv1.norm.running_var', 'res3.2.conv1.norm.weight', 'res3.2.conv2.norm.bias', 'res3.2.conv2.norm.running_mean', 'res3.2.conv2.norm.running_var', 'res3.2.conv2.norm.weight', 'res3.2.conv3.norm.bias', 'res3.2.conv3.norm.running_mean', 'res3.2.conv3.norm.running_var', 'res3.2.conv3.norm.weight', 'res3.2.conv1.weight', 'res3.2.conv2.weight', 'res3.2.conv3.weight', 'res3.3.conv1.norm.bias', 'res3.3.conv1.norm.running_mean', 'res3.3.conv1.norm.running_var', 'res3.3.conv1.norm.weight', 'res3.3.conv2.norm.bias', 'res3.3.conv2.norm.running_mean', 'res3.3.conv2.norm.running_var', 'res3.3.conv2.norm.weight', 'res3.3.conv3.norm.bias', 'res3.3.conv3.norm.running_mean', 'res3.3.conv3.norm.running_var', 'res3.3.conv3.norm.weight', 'res3.3.conv1.weight', 'res3.3.conv2.weight', 'res3.3.conv3.weight', 'res4.0.conv1.norm.bias', 'res4.0.conv1.norm.running_mean', 'res4.0.conv1.norm.running_var', 'res4.0.conv1.norm.weight', 'res4.0.conv2.norm.bias', 'res4.0.conv2.norm.running_mean', 'res4.0.conv2.norm.running_var', 'res4.0.conv2.norm.weight', 'res4.0.conv3.norm.bias', 'res4.0.conv3.norm.running_mean', 'res4.0.conv3.norm.running_var', 'res4.0.conv3.norm.weight', 'res4.0.conv1.weight', 'res4.0.conv2.weight', 'res4.0.conv3.weight', 'res4.0.shortcut.weight', 'res4.0.shortcut.norm.bias', 'res4.0.shortcut.norm.running_mean', 'res4.0.shortcut.norm.running_var', 'res4.0.shortcut.norm.weight', 'res4.1.conv1.norm.bias', 'res4.1.conv1.norm.running_mean', 'res4.1.conv1.norm.running_var', 'res4.1.conv1.norm.weight', 'res4.1.conv2.norm.bias', 'res4.1.conv2.norm.running_mean', 'res4.1.conv2.norm.running_var', 'res4.1.conv2.norm.weight', 'res4.1.conv3.norm.bias', 'res4.1.conv3.norm.running_mean', 'res4.1.conv3.norm.running_var', 'res4.1.conv3.norm.weight', 'res4.1.conv1.weight', 'res4.1.conv2.weight', 'res4.1.conv3.weight', 'res4.2.conv1.norm.bias', 'res4.2.conv1.norm.running_mean', 'res4.2.conv1.norm.running_var', 'res4.2.conv1.norm.weight', 'res4.2.conv2.norm.bias', 'res4.2.conv2.norm.running_mean', 'res4.2.conv2.norm.running_var', 'res4.2.conv2.norm.weight', 'res4.2.conv3.norm.bias', 'res4.2.conv3.norm.running_mean', 'res4.2.conv3.norm.running_var', 'res4.2.conv3.norm.weight', 'res4.2.conv1.weight', 'res4.2.conv2.weight', 'res4.2.conv3.weight', 'res4.3.conv1.norm.bias', 'res4.3.conv1.norm.running_mean', 'res4.3.conv1.norm.running_var', 'res4.3.conv1.norm.weight', 'res4.3.conv2.norm.bias', 'res4.3.conv2.norm.running_mean', 'res4.3.conv2.norm.running_var', 'res4.3.conv2.norm.weight', 'res4.3.conv3.norm.bias', 'res4.3.conv3.norm.running_mean', 'res4.3.conv3.norm.running_var', 'res4.3.conv3.norm.weight', 'res4.3.conv1.weight', 'res4.3.conv2.weight', 'res4.3.conv3.weight', 'res4.4.conv1.norm.bias', 'res4.4.conv1.norm.running_mean', 'res4.4.conv1.norm.running_var', 'res4.4.conv1.norm.weight', 'res4.4.conv2.norm.bias', 'res4.4.conv2.norm.running_mean', 'res4.4.conv2.norm.running_var', 'res4.4.conv2.norm.weight', 'res4.4.conv3.norm.bias', 'res4.4.conv3.norm.running_mean', 'res4.4.conv3.norm.running_var', 'res4.4.conv3.norm.weight', 'res4.4.conv1.weight', 'res4.4.conv2.weight', 'res4.4.conv3.weight', 'res4.5.conv1.norm.bias', 'res4.5.conv1.norm.running_mean', 'res4.5.conv1.norm.running_var', 'res4.5.conv1.norm.weight', 'res4.5.conv2.norm.bias', 'res4.5.conv2.norm.running_mean', 'res4.5.conv2.norm.running_var', 'res4.5.conv2.norm.weight', 'res4.5.conv3.norm.bias', 'res4.5.conv3.norm.running_mean', 'res4.5.conv3.norm.running_var', 'res4.5.conv3.norm.weight', 'res4.5.conv1.weight', 'res4.5.conv2.weight', 'res4.5.conv3.weight', 'res5.0.conv1.norm.bias', 'res5.0.conv1.norm.running_mean', 'res5.0.conv1.norm.running_var', 'res5.0.conv1.norm.weight', 'res5.0.conv2.norm.bias', 'res5.0.conv2.norm.running_mean', 'res5.0.conv2.norm.running_var', 'res5.0.conv2.norm.weight', 'res5.0.conv3.norm.bias', 'res5.0.conv3.norm.running_mean', 'res5.0.conv3.norm.running_var', 'res5.0.conv3.norm.weight', 'res5.0.conv1.weight', 'res5.0.conv2.weight', 'res5.0.conv3.weight', 'res5.0.shortcut.weight', 'res5.0.shortcut.norm.bias', 'res5.0.shortcut.norm.running_mean', 'res5.0.shortcut.norm.running_var', 'res5.0.shortcut.norm.weight', 'res5.1.conv1.norm.bias', 'res5.1.conv1.norm.running_mean', 'res5.1.conv1.norm.running_var', 'res5.1.conv1.norm.weight', 'res5.1.conv2.norm.bias', 'res5.1.conv2.norm.running_mean', 'res5.1.conv2.norm.running_var', 'res5.1.conv2.norm.weight', 'res5.1.conv3.norm.bias', 'res5.1.conv3.norm.running_mean', 'res5.1.conv3.norm.running_var', 'res5.1.conv3.norm.weight', 'res5.1.conv1.weight', 'res5.1.conv2.weight', 'res5.1.conv3.weight', 'res5.2.conv1.norm.bias', 'res5.2.conv1.norm.running_mean', 'res5.2.conv1.norm.running_var', 'res5.2.conv1.norm.weight', 'res5.2.conv2.norm.bias', 'res5.2.conv2.norm.running_mean', 'res5.2.conv2.norm.running_var', 'res5.2.conv2.norm.weight', 'res5.2.conv3.norm.bias', 'res5.2.conv3.norm.running_mean', 'res5.2.conv3.norm.running_var', 'res5.2.conv3.norm.weight', 'res5.2.conv1.weight', 'res5.2.conv2.weight', 'res5.2.conv3.weight'])\n",
      "stem.conv1.norm.bias\n",
      "(64,)\n",
      "stem.conv1.norm.running_mean\n",
      "(64,)\n",
      "stem.conv1.norm.running_var\n",
      "(64,)\n",
      "stem.conv1.norm.weight\n",
      "(64,)\n",
      "stem.conv1.weight\n",
      "(64, 10, 7, 7)\n",
      "stem.fc.bias\n",
      "(19,)\n",
      "stem.fc.weight \n",
      "(19, 2048)\n",
      "res2.0.conv1.norm.bias\n",
      "(64,)\n",
      "res2.0.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.0.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.0.conv1.norm.weight\n",
      "(64,)\n",
      "res2.0.conv2.norm.bias\n",
      "(64,)\n",
      "res2.0.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.0.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.0.conv2.norm.weight\n",
      "(64,)\n",
      "res2.0.conv3.norm.bias\n",
      "(256,)\n",
      "res2.0.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.0.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.0.conv3.norm.weight\n",
      "(256,)\n",
      "res2.0.conv1.weight\n",
      "(64, 64, 1, 1)\n",
      "res2.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.0.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.0.shortcut.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.0.shortcut.norm.bias\n",
      "(256,)\n",
      "res2.0.shortcut.norm.running_mean\n",
      "(256,)\n",
      "res2.0.shortcut.norm.running_var\n",
      "(256,)\n",
      "res2.0.shortcut.norm.weight\n",
      "(256,)\n",
      "res2.1.conv1.norm.bias\n",
      "(64,)\n",
      "res2.1.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.1.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.1.conv1.norm.weight\n",
      "(64,)\n",
      "res2.1.conv2.norm.bias\n",
      "(64,)\n",
      "res2.1.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.1.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.1.conv2.norm.weight\n",
      "(64,)\n",
      "res2.1.conv3.norm.bias\n",
      "(256,)\n",
      "res2.1.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.1.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.1.conv3.norm.weight\n",
      "(256,)\n",
      "res2.1.conv1.weight\n",
      "(64, 256, 1, 1)\n",
      "res2.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.1.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.2.conv1.norm.bias\n",
      "(64,)\n",
      "res2.2.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.2.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.2.conv1.norm.weight\n",
      "(64,)\n",
      "res2.2.conv2.norm.bias\n",
      "(64,)\n",
      "res2.2.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.2.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.2.conv2.norm.weight\n",
      "(64,)\n",
      "res2.2.conv3.norm.bias\n",
      "(256,)\n",
      "res2.2.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.2.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.2.conv3.norm.weight\n",
      "(256,)\n",
      "res2.2.conv1.weight\n",
      "(64, 256, 1, 1)\n",
      "res2.2.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.2.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res3.0.conv1.norm.bias\n",
      "(128,)\n",
      "res3.0.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.0.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.0.conv1.norm.weight\n",
      "(128,)\n",
      "res3.0.conv2.norm.bias\n",
      "(128,)\n",
      "res3.0.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.0.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.0.conv2.norm.weight\n",
      "(128,)\n",
      "res3.0.conv3.norm.bias\n",
      "(512,)\n",
      "res3.0.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.0.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.0.conv3.norm.weight\n",
      "(512,)\n",
      "res3.0.conv1.weight\n",
      "(128, 256, 1, 1)\n",
      "res3.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.0.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.0.shortcut.weight\n",
      "(512, 256, 1, 1)\n",
      "res3.0.shortcut.norm.bias\n",
      "(512,)\n",
      "res3.0.shortcut.norm.running_mean\n",
      "(512,)\n",
      "res3.0.shortcut.norm.running_var\n",
      "(512,)\n",
      "res3.0.shortcut.norm.weight\n",
      "(512,)\n",
      "res3.1.conv1.norm.bias\n",
      "(128,)\n",
      "res3.1.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.1.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.1.conv1.norm.weight\n",
      "(128,)\n",
      "res3.1.conv2.norm.bias\n",
      "(128,)\n",
      "res3.1.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.1.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.1.conv2.norm.weight\n",
      "(128,)\n",
      "res3.1.conv3.norm.bias\n",
      "(512,)\n",
      "res3.1.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.1.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.1.conv3.norm.weight\n",
      "(512,)\n",
      "res3.1.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.1.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.2.conv1.norm.bias\n",
      "(128,)\n",
      "res3.2.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.2.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.2.conv1.norm.weight\n",
      "(128,)\n",
      "res3.2.conv2.norm.bias\n",
      "(128,)\n",
      "res3.2.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.2.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.2.conv2.norm.weight\n",
      "(128,)\n",
      "res3.2.conv3.norm.bias\n",
      "(512,)\n",
      "res3.2.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.2.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.2.conv3.norm.weight\n",
      "(512,)\n",
      "res3.2.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.2.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.2.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.3.conv1.norm.bias\n",
      "(128,)\n",
      "res3.3.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.3.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.3.conv1.norm.weight\n",
      "(128,)\n",
      "res3.3.conv2.norm.bias\n",
      "(128,)\n",
      "res3.3.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.3.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.3.conv2.norm.weight\n",
      "(128,)\n",
      "res3.3.conv3.norm.bias\n",
      "(512,)\n",
      "res3.3.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.3.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.3.conv3.norm.weight\n",
      "(512,)\n",
      "res3.3.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.3.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.3.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res4.0.conv1.norm.bias\n",
      "(256,)\n",
      "res4.0.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.0.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.0.conv1.norm.weight\n",
      "(256,)\n",
      "res4.0.conv2.norm.bias\n",
      "(256,)\n",
      "res4.0.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.0.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.0.conv2.norm.weight\n",
      "(256,)\n",
      "res4.0.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.0.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.0.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.0.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.0.conv1.weight\n",
      "(256, 512, 1, 1)\n",
      "res4.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.0.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.0.shortcut.weight\n",
      "(1024, 512, 1, 1)\n",
      "res4.0.shortcut.norm.bias\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.running_mean\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.running_var\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.weight\n",
      "(1024,)\n",
      "res4.1.conv1.norm.bias\n",
      "(256,)\n",
      "res4.1.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.1.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.1.conv1.norm.weight\n",
      "(256,)\n",
      "res4.1.conv2.norm.bias\n",
      "(256,)\n",
      "res4.1.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.1.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.1.conv2.norm.weight\n",
      "(256,)\n",
      "res4.1.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.1.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.1.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.1.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.1.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.1.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.2.conv1.norm.bias\n",
      "(256,)\n",
      "res4.2.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.2.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.2.conv1.norm.weight\n",
      "(256,)\n",
      "res4.2.conv2.norm.bias\n",
      "(256,)\n",
      "res4.2.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.2.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.2.conv2.norm.weight\n",
      "(256,)\n",
      "res4.2.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.2.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.2.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.2.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.2.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.2.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.2.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.3.conv1.norm.bias\n",
      "(256,)\n",
      "res4.3.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.3.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.3.conv1.norm.weight\n",
      "(256,)\n",
      "res4.3.conv2.norm.bias\n",
      "(256,)\n",
      "res4.3.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.3.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.3.conv2.norm.weight\n",
      "(256,)\n",
      "res4.3.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.3.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.3.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.3.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.3.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.3.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.3.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.4.conv1.norm.bias\n",
      "(256,)\n",
      "res4.4.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.4.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.4.conv1.norm.weight\n",
      "(256,)\n",
      "res4.4.conv2.norm.bias\n",
      "(256,)\n",
      "res4.4.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.4.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.4.conv2.norm.weight\n",
      "(256,)\n",
      "res4.4.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.4.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.4.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.4.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.4.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.4.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.4.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.5.conv1.norm.bias\n",
      "(256,)\n",
      "res4.5.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.5.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.5.conv1.norm.weight\n",
      "(256,)\n",
      "res4.5.conv2.norm.bias\n",
      "(256,)\n",
      "res4.5.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.5.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.5.conv2.norm.weight\n",
      "(256,)\n",
      "res4.5.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.5.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.5.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.5.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.5.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.5.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.5.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res5.0.conv1.norm.bias\n",
      "(512,)\n",
      "res5.0.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.0.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.0.conv1.norm.weight\n",
      "(512,)\n",
      "res5.0.conv2.norm.bias\n",
      "(512,)\n",
      "res5.0.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.0.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.0.conv2.norm.weight\n",
      "(512,)\n",
      "res5.0.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.0.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.0.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.0.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.0.conv1.weight\n",
      "(512, 1024, 1, 1)\n",
      "res5.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.0.conv3.weight\n",
      "(2048, 512, 1, 1)\n",
      "res5.0.shortcut.weight\n",
      "(2048, 1024, 1, 1)\n",
      "res5.0.shortcut.norm.bias\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.running_mean\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.running_var\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.weight\n",
      "(2048,)\n",
      "res5.1.conv1.norm.bias\n",
      "(512,)\n",
      "res5.1.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.1.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.1.conv1.norm.weight\n",
      "(512,)\n",
      "res5.1.conv2.norm.bias\n",
      "(512,)\n",
      "res5.1.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.1.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.1.conv2.norm.weight\n",
      "(512,)\n",
      "res5.1.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.1.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.1.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.1.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.1.conv1.weight\n",
      "(512, 2048, 1, 1)\n",
      "res5.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.1.conv3.weight\n",
      "(2048, 512, 1, 1)\n",
      "res5.2.conv1.norm.bias\n",
      "(512,)\n",
      "res5.2.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.2.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.2.conv1.norm.weight\n",
      "(512,)\n",
      "res5.2.conv2.norm.bias\n",
      "(512,)\n",
      "res5.2.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.2.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.2.conv2.norm.weight\n",
      "(512,)\n",
      "res5.2.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.2.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.2.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.2.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.2.conv1.weight\n",
      "(512, 2048, 1, 1)\n",
      "res5.2.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.2.conv3.weight\n",
      "(2048, 512, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# load the new pickle file\n",
    "with open(new_pkl_path, 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "\n",
    "print(weights.keys())\n",
    "print(weights[\"model\"].keys())\n",
    "\n",
    "for key in weights[\"model\"].keys():\n",
    "    print(key)\n",
    "    print(weights[\"model\"][key].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segn la traza del entrenamiento le faltan estos pesos\n",
    "\n",
    "backbone.res2.0.conv3.norm.bias\n",
    "backbone.res2.1.conv3.norm.bias\n",
    "backbone.res2.2.conv3.norm.bias\n",
    "backbone.res3.0.conv3.norm.bias\n",
    "backbone.res3.1.conv3.norm.bias\n",
    "backbone.res3.2.conv3.norm.bias\n",
    "backbone.res3.3.conv3.norm.bias\n",
    "backbone.res4.0.conv3.norm.bias\n",
    "backbone.res4.1.conv3.norm.bias\n",
    "backbone.res4.2.conv3.norm.bias\n",
    "backbone.res4.3.conv3.norm.bias\n",
    "backbone.res4.4.conv3.norm.bias\n",
    "backbone.res4.5.conv3.norm.bias\n",
    "backbone.res5.0.conv3.norm.bias\n",
    "backbone.res5.1.conv3.norm.bias\n",
    "backbone.res5.2.conv3.norm.bias\n",
    "backbone.stem.conv1.norm.bias\n",
    "backbone.stem.conv1.weight\n",
    "\n",
    "criterion.empty_weight\n",
    "\n",
    "\n",
    "WARNING [10/25 09:18:33 fvcore.common.checkpoint]: The checkpoint state_dict contains keys that are not used by the model:\n",
    "  stem.conv1.weight\n",
    "  stem.fc.{bias, weight }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem.conv1.norm.bias\n",
      "(64,)\n",
      "stem.conv1.norm.running_mean\n",
      "(64,)\n",
      "stem.conv1.norm.running_var\n",
      "(64,)\n",
      "stem.conv1.norm.weight\n",
      "(64,)\n",
      "stem.conv1.weight\n",
      "(64, 12, 7, 7)\n",
      "stem.fc.bias\n",
      "(19,)\n",
      "stem.fc.weight \n",
      "(19, 2048)\n",
      "res2.0.conv1.norm.bias\n",
      "(64,)\n",
      "res2.0.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.0.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.0.conv1.norm.weight\n",
      "(64,)\n",
      "res2.0.conv2.norm.bias\n",
      "(64,)\n",
      "res2.0.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.0.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.0.conv2.norm.weight\n",
      "(64,)\n",
      "res2.0.conv3.norm.bias\n",
      "(256,)\n",
      "res2.0.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.0.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.0.conv3.norm.weight\n",
      "(256,)\n",
      "res2.0.conv1.weight\n",
      "(64, 64, 1, 1)\n",
      "res2.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.0.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.0.shortcut.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.0.shortcut.norm.bias\n",
      "(256,)\n",
      "res2.0.shortcut.norm.running_mean\n",
      "(256,)\n",
      "res2.0.shortcut.norm.running_var\n",
      "(256,)\n",
      "res2.0.shortcut.norm.weight\n",
      "(256,)\n",
      "res2.1.conv1.norm.bias\n",
      "(64,)\n",
      "res2.1.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.1.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.1.conv1.norm.weight\n",
      "(64,)\n",
      "res2.1.conv2.norm.bias\n",
      "(64,)\n",
      "res2.1.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.1.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.1.conv2.norm.weight\n",
      "(64,)\n",
      "res2.1.conv3.norm.bias\n",
      "(256,)\n",
      "res2.1.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.1.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.1.conv3.norm.weight\n",
      "(256,)\n",
      "res2.1.conv1.weight\n",
      "(64, 256, 1, 1)\n",
      "res2.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.1.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.2.conv1.norm.bias\n",
      "(64,)\n",
      "res2.2.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.2.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.2.conv1.norm.weight\n",
      "(64,)\n",
      "res2.2.conv2.norm.bias\n",
      "(64,)\n",
      "res2.2.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.2.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.2.conv2.norm.weight\n",
      "(64,)\n",
      "res2.2.conv3.norm.bias\n",
      "(256,)\n",
      "res2.2.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.2.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.2.conv3.norm.weight\n",
      "(256,)\n",
      "res2.2.conv1.weight\n",
      "(64, 256, 1, 1)\n",
      "res2.2.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.2.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res3.0.conv1.norm.bias\n",
      "(128,)\n",
      "res3.0.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.0.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.0.conv1.norm.weight\n",
      "(128,)\n",
      "res3.0.conv2.norm.bias\n",
      "(128,)\n",
      "res3.0.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.0.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.0.conv2.norm.weight\n",
      "(128,)\n",
      "res3.0.conv3.norm.bias\n",
      "(512,)\n",
      "res3.0.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.0.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.0.conv3.norm.weight\n",
      "(512,)\n",
      "res3.0.conv1.weight\n",
      "(128, 256, 1, 1)\n",
      "res3.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.0.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.0.shortcut.weight\n",
      "(512, 256, 1, 1)\n",
      "res3.0.shortcut.norm.bias\n",
      "(512,)\n",
      "res3.0.shortcut.norm.running_mean\n",
      "(512,)\n",
      "res3.0.shortcut.norm.running_var\n",
      "(512,)\n",
      "res3.0.shortcut.norm.weight\n",
      "(512,)\n",
      "res3.1.conv1.norm.bias\n",
      "(128,)\n",
      "res3.1.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.1.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.1.conv1.norm.weight\n",
      "(128,)\n",
      "res3.1.conv2.norm.bias\n",
      "(128,)\n",
      "res3.1.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.1.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.1.conv2.norm.weight\n",
      "(128,)\n",
      "res3.1.conv3.norm.bias\n",
      "(512,)\n",
      "res3.1.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.1.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.1.conv3.norm.weight\n",
      "(512,)\n",
      "res3.1.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.1.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.2.conv1.norm.bias\n",
      "(128,)\n",
      "res3.2.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.2.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.2.conv1.norm.weight\n",
      "(128,)\n",
      "res3.2.conv2.norm.bias\n",
      "(128,)\n",
      "res3.2.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.2.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.2.conv2.norm.weight\n",
      "(128,)\n",
      "res3.2.conv3.norm.bias\n",
      "(512,)\n",
      "res3.2.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.2.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.2.conv3.norm.weight\n",
      "(512,)\n",
      "res3.2.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.2.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.2.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.3.conv1.norm.bias\n",
      "(128,)\n",
      "res3.3.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.3.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.3.conv1.norm.weight\n",
      "(128,)\n",
      "res3.3.conv2.norm.bias\n",
      "(128,)\n",
      "res3.3.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.3.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.3.conv2.norm.weight\n",
      "(128,)\n",
      "res3.3.conv3.norm.bias\n",
      "(512,)\n",
      "res3.3.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.3.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.3.conv3.norm.weight\n",
      "(512,)\n",
      "res3.3.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.3.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.3.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res4.0.conv1.norm.bias\n",
      "(256,)\n",
      "res4.0.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.0.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.0.conv1.norm.weight\n",
      "(256,)\n",
      "res4.0.conv2.norm.bias\n",
      "(256,)\n",
      "res4.0.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.0.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.0.conv2.norm.weight\n",
      "(256,)\n",
      "res4.0.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.0.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.0.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.0.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.0.conv1.weight\n",
      "(256, 512, 1, 1)\n",
      "res4.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.0.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.0.shortcut.weight\n",
      "(1024, 512, 1, 1)\n",
      "res4.0.shortcut.norm.bias\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.running_mean\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.running_var\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.weight\n",
      "(1024,)\n",
      "res4.1.conv1.norm.bias\n",
      "(256,)\n",
      "res4.1.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.1.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.1.conv1.norm.weight\n",
      "(256,)\n",
      "res4.1.conv2.norm.bias\n",
      "(256,)\n",
      "res4.1.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.1.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.1.conv2.norm.weight\n",
      "(256,)\n",
      "res4.1.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.1.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.1.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.1.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.1.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.1.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.2.conv1.norm.bias\n",
      "(256,)\n",
      "res4.2.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.2.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.2.conv1.norm.weight\n",
      "(256,)\n",
      "res4.2.conv2.norm.bias\n",
      "(256,)\n",
      "res4.2.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.2.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.2.conv2.norm.weight\n",
      "(256,)\n",
      "res4.2.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.2.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.2.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.2.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.2.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.2.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.2.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.3.conv1.norm.bias\n",
      "(256,)\n",
      "res4.3.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.3.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.3.conv1.norm.weight\n",
      "(256,)\n",
      "res4.3.conv2.norm.bias\n",
      "(256,)\n",
      "res4.3.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.3.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.3.conv2.norm.weight\n",
      "(256,)\n",
      "res4.3.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.3.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.3.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.3.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.3.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.3.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.3.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.4.conv1.norm.bias\n",
      "(256,)\n",
      "res4.4.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.4.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.4.conv1.norm.weight\n",
      "(256,)\n",
      "res4.4.conv2.norm.bias\n",
      "(256,)\n",
      "res4.4.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.4.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.4.conv2.norm.weight\n",
      "(256,)\n",
      "res4.4.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.4.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.4.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.4.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.4.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.4.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.4.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.5.conv1.norm.bias\n",
      "(256,)\n",
      "res4.5.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.5.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.5.conv1.norm.weight\n",
      "(256,)\n",
      "res4.5.conv2.norm.bias\n",
      "(256,)\n",
      "res4.5.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.5.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.5.conv2.norm.weight\n",
      "(256,)\n",
      "res4.5.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.5.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.5.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.5.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.5.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.5.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.5.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res5.0.conv1.norm.bias\n",
      "(512,)\n",
      "res5.0.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.0.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.0.conv1.norm.weight\n",
      "(512,)\n",
      "res5.0.conv2.norm.bias\n",
      "(512,)\n",
      "res5.0.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.0.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.0.conv2.norm.weight\n",
      "(512,)\n",
      "res5.0.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.0.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.0.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.0.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.0.conv1.weight\n",
      "(512, 1024, 1, 1)\n",
      "res5.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.0.conv3.weight\n",
      "(2048, 512, 1, 1)\n",
      "res5.0.shortcut.weight\n",
      "(2048, 1024, 1, 1)\n",
      "res5.0.shortcut.norm.bias\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.running_mean\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.running_var\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.weight\n",
      "(2048,)\n",
      "res5.1.conv1.norm.bias\n",
      "(512,)\n",
      "res5.1.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.1.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.1.conv1.norm.weight\n",
      "(512,)\n",
      "res5.1.conv2.norm.bias\n",
      "(512,)\n",
      "res5.1.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.1.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.1.conv2.norm.weight\n",
      "(512,)\n",
      "res5.1.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.1.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.1.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.1.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.1.conv1.weight\n",
      "(512, 2048, 1, 1)\n",
      "res5.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.1.conv3.weight\n",
      "(2048, 512, 1, 1)\n",
      "res5.2.conv1.norm.bias\n",
      "(512,)\n",
      "res5.2.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.2.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.2.conv1.norm.weight\n",
      "(512,)\n",
      "res5.2.conv2.norm.bias\n",
      "(512,)\n",
      "res5.2.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.2.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.2.conv2.norm.weight\n",
      "(512,)\n",
      "res5.2.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.2.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.2.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.2.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.2.conv1.weight\n",
      "(512, 2048, 1, 1)\n",
      "res5.2.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.2.conv3.weight\n",
      "(2048, 512, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mask2formerResnetPklPath = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/R-50.pkl'\n",
    "BigEarthResnetPthPath = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/model_s1+2.safetensors'\n",
    "\n",
    "# Keep the Bigearth weights in a dictionary with the keys from the Mask2Former model\n",
    "bigearth_weights = {}\n",
    "with safe_open(BigEarthResnetPthPath, framework=\"pt\") as f:\n",
    "    for key in f.keys():\n",
    "        if key in dictionary_from_Sentinel_to_Mask2former:\n",
    "            new_key = dictionary_from_Sentinel_to_Mask2former[key]\n",
    "            bigearth_weights[new_key] = np.array(f.get_tensor(key))\n",
    "\n",
    "for key in bigearth_weights.keys():\n",
    "    print(key)\n",
    "    print(bigearth_weights[key].shape)\n",
    "\n",
    "# Load the weights from the pickle file\n",
    "with open(mask2formerResnetPklPath, 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "\n",
    "weights[\"model\"] = bigearth_weights\n",
    "\n",
    "# Save the updated weights to a new pickle file\n",
    "new_pkl_path = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/R-50_S2+1.pkl'\n",
    "with open(new_pkl_path, 'wb') as f:\n",
    "    pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Order of bands for the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_from_Sentinel_to_Mask2former = {\n",
    "    \"model.vision_encoder.fc.weight\":\"stem.fc.weight \",\n",
    "    \"model.vision_encoder.fc.bias\":\"stem.fc.bias\",\n",
    "\n",
    "    #STEM\n",
    "    #\"model.vision_encoder.conv1.weight\":\"stem.conv1.weight\", # Esta es la que tengo que ajustar a mano # el shape que tiene es (64, 8, 7, 7), tengo que convertirlo a (64, 4, 7, 7) teniendo en cuenta el orden correcto de bandas\n",
    "    \"model.vision_encoder.bn1.running_mean\":\"stem.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.bn1.running_var\":\"stem.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.bn1.weight\":\"stem.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.bn1.bias\":\"stem.conv1.norm.bias\",\n",
    "    \n",
    "    # RES2 Block Layer0\n",
    "    \"model.vision_encoder.layer1.0.conv1.weight\":\"res2.0.conv1.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn1.running_mean\":\"res2.0.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.0.bn1.running_var\":\"res2.0.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.0.bn1.weight\":\"res2.0.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn1.bias\":\"res2.0.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.0.conv2.weight\":\"res2.0.conv2.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn2.running_mean\":\"res2.0.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.0.bn2.running_var\":\"res2.0.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.0.bn2.weight\":\"res2.0.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn2.bias\":\"res2.0.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.0.conv3.weight\":\"res2.0.conv3.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn3.running_mean\":\"res2.0.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.0.bn3.running_var\":\"res2.0.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.0.bn3.weight\":\"res2.0.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.0.bn3.bias\":\"res2.0.conv3.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.0.downsample.0.weight\":\"res2.0.shortcut.weight\",\n",
    "    \"model.vision_encoder.layer1.0.downsample.1.running_mean\":\"res2.0.shortcut.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.0.downsample.1.running_var\":\"res2.0.shortcut.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.0.downsample.1.weight\":\"res2.0.shortcut.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.0.downsample.1.bias\":\"res2.0.shortcut.norm.bias\",\n",
    "\n",
    "    # RES2 Block Layer1\n",
    "    \"model.vision_encoder.layer1.1.conv1.weight\":\"res2.1.conv1.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn1.running_mean\":\"res2.1.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.1.bn1.running_var\":\"res2.1.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.1.bn1.weight\":\"res2.1.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn1.bias\":\"res2.1.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.1.conv2.weight\":\"res2.1.conv2.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn2.running_mean\":\"res2.1.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.1.bn2.running_var\":\"res2.1.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.1.bn2.weight\":\"res2.1.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn2.bias\":\"res2.1.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.1.conv3.weight\":\"res2.1.conv3.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn3.running_mean\":\"res2.1.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.1.bn3.running_var\":\"res2.1.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.1.bn3.weight\":\"res2.1.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.1.bn3.bias\":\"res2.1.conv3.norm.bias\",\n",
    "\n",
    "    # RES2 Block Layer2\n",
    "    \"model.vision_encoder.layer1.2.conv1.weight\":\"res2.2.conv1.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn1.running_mean\":\"res2.2.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.2.bn1.running_var\":\"res2.2.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.2.bn1.weight\":\"res2.2.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn1.bias\":\"res2.2.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.2.conv2.weight\":\"res2.2.conv2.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn2.running_mean\":\"res2.2.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.2.bn2.running_var\":\"res2.2.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.2.bn2.weight\":\"res2.2.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn2.bias\":\"res2.2.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer1.2.conv3.weight\":\"res2.2.conv3.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn3.running_mean\":\"res2.2.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer1.2.bn3.running_var\":\"res2.2.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer1.2.bn3.weight\":\"res2.2.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer1.2.bn3.bias\":\"res2.2.conv3.norm.bias\",\n",
    "\n",
    "    # RES3 Block Layer0\n",
    "    \"model.vision_encoder.layer2.0.conv1.weight\":\"res3.0.conv1.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn1.running_mean\":\"res3.0.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.0.bn1.running_var\":\"res3.0.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.0.bn1.weight\":\"res3.0.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn1.bias\":\"res3.0.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.0.conv2.weight\":\"res3.0.conv2.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn2.running_mean\":\"res3.0.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.0.bn2.running_var\":\"res3.0.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.0.bn2.weight\":\"res3.0.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn2.bias\":\"res3.0.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.0.conv3.weight\":\"res3.0.conv3.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn3.running_mean\":\"res3.0.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.0.bn3.running_var\":\"res3.0.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.0.bn3.weight\":\"res3.0.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.0.bn3.bias\":\"res3.0.conv3.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.0.downsample.0.weight\":\"res3.0.shortcut.weight\",\n",
    "    \"model.vision_encoder.layer2.0.downsample.1.running_mean\":\"res3.0.shortcut.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.0.downsample.1.running_var\":\"res3.0.shortcut.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.0.downsample.1.weight\":\"res3.0.shortcut.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.0.downsample.1.bias\":\"res3.0.shortcut.norm.bias\",\n",
    "\n",
    "    # RES3 Block Layer1\n",
    "    \"model.vision_encoder.layer2.1.conv1.weight\":\"res3.1.conv1.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn1.running_mean\":\"res3.1.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.1.bn1.running_var\":\"res3.1.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.1.bn1.weight\":\"res3.1.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn1.bias\":\"res3.1.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.1.conv2.weight\":\"res3.1.conv2.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn2.running_mean\":\"res3.1.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.1.bn2.running_var\":\"res3.1.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.1.bn2.weight\":\"res3.1.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn2.bias\":\"res3.1.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.1.conv3.weight\":\"res3.1.conv3.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn3.running_mean\":\"res3.1.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.1.bn3.running_var\":\"res3.1.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.1.bn3.weight\":\"res3.1.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.1.bn3.bias\":\"res3.1.conv3.norm.bias\",\n",
    "\n",
    "    # RES3 Block Layer2\n",
    "    \"model.vision_encoder.layer2.2.conv1.weight\":\"res3.2.conv1.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn1.running_mean\":\"res3.2.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.2.bn1.running_var\":\"res3.2.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.2.bn1.weight\":\"res3.2.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn1.bias\":\"res3.2.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.2.conv2.weight\":\"res3.2.conv2.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn2.running_mean\":\"res3.2.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.2.bn2.running_var\":\"res3.2.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.2.bn2.weight\":\"res3.2.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn2.bias\":\"res3.2.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.2.conv3.weight\":\"res3.2.conv3.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn3.running_mean\":\"res3.2.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.2.bn3.running_var\":\"res3.2.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.2.bn3.weight\":\"res3.2.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.2.bn3.bias\":\"res3.2.conv3.norm.bias\",\n",
    "\n",
    "    # RES3 Block Layer3\n",
    "    \"model.vision_encoder.layer2.3.conv1.weight\":\"res3.3.conv1.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn1.running_mean\":\"res3.3.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.3.bn1.running_var\":\"res3.3.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.3.bn1.weight\":\"res3.3.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn1.bias\":\"res3.3.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.3.conv2.weight\":\"res3.3.conv2.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn2.running_mean\":\"res3.3.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.3.bn2.running_var\":\"res3.3.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.3.bn2.weight\":\"res3.3.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn2.bias\":\"res3.3.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer2.3.conv3.weight\":\"res3.3.conv3.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn3.running_mean\":\"res3.3.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer2.3.bn3.running_var\":\"res3.3.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer2.3.bn3.weight\":\"res3.3.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer2.3.bn3.bias\":\"res3.3.conv3.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer0\n",
    "    \"model.vision_encoder.layer3.0.conv1.weight\":\"res4.0.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn1.running_mean\":\"res4.0.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.0.bn1.running_var\":\"res4.0.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.0.bn1.weight\":\"res4.0.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn1.bias\":\"res4.0.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.0.conv2.weight\":\"res4.0.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn2.running_mean\":\"res4.0.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.0.bn2.running_var\":\"res4.0.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.0.bn2.weight\":\"res4.0.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn2.bias\":\"res4.0.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.0.conv3.weight\":\"res4.0.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn3.running_mean\":\"res4.0.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.0.bn3.running_var\":\"res4.0.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.0.bn3.weight\":\"res4.0.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.0.bn3.bias\":\"res4.0.conv3.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.0.downsample.0.weight\":\"res4.0.shortcut.weight\",\n",
    "    \"model.vision_encoder.layer3.0.downsample.1.running_mean\":\"res4.0.shortcut.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.0.downsample.1.running_var\":\"res4.0.shortcut.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.0.downsample.1.weight\":\"res4.0.shortcut.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.0.downsample.1.bias\":\"res4.0.shortcut.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer1\n",
    "    \"model.vision_encoder.layer3.1.conv1.weight\":\"res4.1.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn1.running_mean\":\"res4.1.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.1.bn1.running_var\":\"res4.1.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.1.bn1.weight\":\"res4.1.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn1.bias\":\"res4.1.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.1.conv2.weight\":\"res4.1.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn2.running_mean\":\"res4.1.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.1.bn2.running_var\":\"res4.1.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.1.bn2.weight\":\"res4.1.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn2.bias\":\"res4.1.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.1.conv3.weight\":\"res4.1.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn3.running_mean\":\"res4.1.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.1.bn3.running_var\":\"res4.1.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.1.bn3.weight\":\"res4.1.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.1.bn3.bias\":\"res4.1.conv3.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer2\n",
    "    \"model.vision_encoder.layer3.2.conv1.weight\":\"res4.2.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn1.running_mean\":\"res4.2.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.2.bn1.running_var\":\"res4.2.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.2.bn1.weight\":\"res4.2.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn1.bias\":\"res4.2.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.2.conv2.weight\":\"res4.2.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn2.running_mean\":\"res4.2.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.2.bn2.running_var\":\"res4.2.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.2.bn2.weight\":\"res4.2.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn2.bias\":\"res4.2.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.2.conv3.weight\":\"res4.2.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn3.running_mean\":\"res4.2.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.2.bn3.running_var\":\"res4.2.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.2.bn3.weight\":\"res4.2.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.2.bn3.bias\":\"res4.2.conv3.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer3\n",
    "    \"model.vision_encoder.layer3.3.conv1.weight\":\"res4.3.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn1.running_mean\":\"res4.3.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.3.bn1.running_var\":\"res4.3.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.3.bn1.weight\":\"res4.3.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn1.bias\":\"res4.3.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.3.conv2.weight\":\"res4.3.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn2.running_mean\":\"res4.3.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.3.bn2.running_var\":\"res4.3.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.3.bn2.weight\":\"res4.3.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn2.bias\":\"res4.3.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.3.conv3.weight\":\"res4.3.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn3.running_mean\":\"res4.3.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.3.bn3.running_var\":\"res4.3.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.3.bn3.weight\":\"res4.3.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.3.bn3.bias\":\"res4.3.conv3.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer4\n",
    "    \"model.vision_encoder.layer3.4.conv1.weight\":\"res4.4.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn1.running_mean\":\"res4.4.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.4.bn1.running_var\":\"res4.4.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.4.bn1.weight\":\"res4.4.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn1.bias\":\"res4.4.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.4.conv2.weight\":\"res4.4.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn2.running_mean\":\"res4.4.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.4.bn2.running_var\":\"res4.4.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.4.bn2.weight\":\"res4.4.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn2.bias\":\"res4.4.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.4.conv3.weight\":\"res4.4.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn3.running_mean\":\"res4.4.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.4.bn3.running_var\":\"res4.4.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.4.bn3.weight\":\"res4.4.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.4.bn3.bias\":\"res4.4.conv3.norm.bias\",\n",
    "\n",
    "    # RES4 Block Layer5\n",
    "    \"model.vision_encoder.layer3.5.conv1.weight\":\"res4.5.conv1.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn1.running_mean\":\"res4.5.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.5.bn1.running_var\":\"res4.5.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.5.bn1.weight\":\"res4.5.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn1.bias\":\"res4.5.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.5.conv2.weight\":\"res4.5.conv2.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn2.running_mean\":\"res4.5.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.5.bn2.running_var\":\"res4.5.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.5.bn2.weight\":\"res4.5.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn2.bias\":\"res4.5.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer3.5.conv3.weight\":\"res4.5.conv3.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn3.running_mean\":\"res4.5.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer3.5.bn3.running_var\":\"res4.5.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer3.5.bn3.weight\":\"res4.5.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer3.5.bn3.bias\":\"res4.5.conv3.norm.bias\",\n",
    "\n",
    "    # RES5 Block Layer0\n",
    "    \"model.vision_encoder.layer4.0.conv1.weight\":\"res5.0.conv1.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn1.running_mean\":\"res5.0.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.0.bn1.running_var\":\"res5.0.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.0.bn1.weight\":\"res5.0.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn1.bias\":\"res5.0.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.0.conv2.weight\":\"res5.0.conv2.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn2.running_mean\":\"res5.0.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.0.bn2.running_var\":\"res5.0.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.0.bn2.weight\":\"res5.0.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn2.bias\":\"res5.0.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.0.conv3.weight\":\"res5.0.conv3.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn3.running_mean\":\"res5.0.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.0.bn3.running_var\":\"res5.0.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.0.bn3.weight\":\"res5.0.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.0.bn3.bias\":\"res5.0.conv3.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.0.downsample.0.weight\":\"res5.0.shortcut.weight\",\n",
    "    \"model.vision_encoder.layer4.0.downsample.1.running_mean\":\"res5.0.shortcut.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.0.downsample.1.running_var\":\"res5.0.shortcut.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.0.downsample.1.weight\":\"res5.0.shortcut.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.0.downsample.1.bias\":\"res5.0.shortcut.norm.bias\",\n",
    "\n",
    "    # RES5 Block Layer1\n",
    "    \"model.vision_encoder.layer4.1.conv1.weight\":\"res5.1.conv1.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn1.running_mean\":\"res5.1.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.1.bn1.running_var\":\"res5.1.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.1.bn1.weight\":\"res5.1.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn1.bias\":\"res5.1.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.1.conv2.weight\":\"res5.1.conv2.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn2.running_mean\":\"res5.1.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.1.bn2.running_var\":\"res5.1.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.1.bn2.weight\":\"res5.1.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn2.bias\":\"res5.1.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.1.conv3.weight\":\"res5.1.conv3.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn3.running_mean\":\"res5.1.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.1.bn3.running_var\":\"res5.1.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.1.bn3.weight\":\"res5.1.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.1.bn3.bias\":\"res5.1.conv3.norm.bias\",\n",
    "\n",
    "    # RES5 Block Layer2\n",
    "    \"model.vision_encoder.layer4.2.conv1.weight\":\"res5.2.conv1.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn1.running_mean\":\"res5.2.conv1.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.2.bn1.running_var\":\"res5.2.conv1.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.2.bn1.weight\":\"res5.2.conv1.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn1.bias\":\"res5.2.conv1.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.2.conv2.weight\":\"res5.2.conv2.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn2.running_mean\":\"res5.2.conv2.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.2.bn2.running_var\":\"res5.2.conv2.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.2.bn2.weight\":\"res5.2.conv2.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn2.bias\":\"res5.2.conv2.norm.bias\",\n",
    "    \"model.vision_encoder.layer4.2.conv3.weight\":\"res5.2.conv3.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn3.running_mean\":\"res5.2.conv3.norm.running_mean\",\n",
    "    \"model.vision_encoder.layer4.2.bn3.running_var\":\"res5.2.conv3.norm.running_var\",\n",
    "    \"model.vision_encoder.layer4.2.bn3.weight\":\"res5.2.conv3.norm.weight\",\n",
    "    \"model.vision_encoder.layer4.2.bn3.bias\":\"res5.2.conv3.norm.bias\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4, 7, 7)\n",
      "stem.conv1.norm.bias\n",
      "(64,)\n",
      "stem.conv1.norm.running_mean\n",
      "(64,)\n",
      "stem.conv1.norm.running_var\n",
      "(64,)\n",
      "stem.conv1.norm.weight\n",
      "(64,)\n",
      "stem.fc.bias\n",
      "(19,)\n",
      "stem.fc.weight \n",
      "(19, 2048)\n",
      "res2.0.conv1.norm.bias\n",
      "(64,)\n",
      "res2.0.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.0.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.0.conv1.norm.weight\n",
      "(64,)\n",
      "res2.0.conv2.norm.bias\n",
      "(64,)\n",
      "res2.0.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.0.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.0.conv2.norm.weight\n",
      "(64,)\n",
      "res2.0.conv3.norm.bias\n",
      "(256,)\n",
      "res2.0.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.0.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.0.conv3.norm.weight\n",
      "(256,)\n",
      "res2.0.conv1.weight\n",
      "(64, 64, 1, 1)\n",
      "res2.0.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.0.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.0.shortcut.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.0.shortcut.norm.bias\n",
      "(256,)\n",
      "res2.0.shortcut.norm.running_mean\n",
      "(256,)\n",
      "res2.0.shortcut.norm.running_var\n",
      "(256,)\n",
      "res2.0.shortcut.norm.weight\n",
      "(256,)\n",
      "res2.1.conv1.norm.bias\n",
      "(64,)\n",
      "res2.1.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.1.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.1.conv1.norm.weight\n",
      "(64,)\n",
      "res2.1.conv2.norm.bias\n",
      "(64,)\n",
      "res2.1.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.1.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.1.conv2.norm.weight\n",
      "(64,)\n",
      "res2.1.conv3.norm.bias\n",
      "(256,)\n",
      "res2.1.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.1.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.1.conv3.norm.weight\n",
      "(256,)\n",
      "res2.1.conv1.weight\n",
      "(64, 256, 1, 1)\n",
      "res2.1.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.1.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res2.2.conv1.norm.bias\n",
      "(64,)\n",
      "res2.2.conv1.norm.running_mean\n",
      "(64,)\n",
      "res2.2.conv1.norm.running_var\n",
      "(64,)\n",
      "res2.2.conv1.norm.weight\n",
      "(64,)\n",
      "res2.2.conv2.norm.bias\n",
      "(64,)\n",
      "res2.2.conv2.norm.running_mean\n",
      "(64,)\n",
      "res2.2.conv2.norm.running_var\n",
      "(64,)\n",
      "res2.2.conv2.norm.weight\n",
      "(64,)\n",
      "res2.2.conv3.norm.bias\n",
      "(256,)\n",
      "res2.2.conv3.norm.running_mean\n",
      "(256,)\n",
      "res2.2.conv3.norm.running_var\n",
      "(256,)\n",
      "res2.2.conv3.norm.weight\n",
      "(256,)\n",
      "res2.2.conv1.weight\n",
      "(64, 256, 1, 1)\n",
      "res2.2.conv2.weight\n",
      "(64, 64, 3, 3)\n",
      "res2.2.conv3.weight\n",
      "(256, 64, 1, 1)\n",
      "res3.0.conv1.norm.bias\n",
      "(128,)\n",
      "res3.0.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.0.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.0.conv1.norm.weight\n",
      "(128,)\n",
      "res3.0.conv2.norm.bias\n",
      "(128,)\n",
      "res3.0.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.0.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.0.conv2.norm.weight\n",
      "(128,)\n",
      "res3.0.conv3.norm.bias\n",
      "(512,)\n",
      "res3.0.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.0.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.0.conv3.norm.weight\n",
      "(512,)\n",
      "res3.0.conv1.weight\n",
      "(128, 256, 1, 1)\n",
      "res3.0.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.0.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.0.shortcut.weight\n",
      "(512, 256, 1, 1)\n",
      "res3.0.shortcut.norm.bias\n",
      "(512,)\n",
      "res3.0.shortcut.norm.running_mean\n",
      "(512,)\n",
      "res3.0.shortcut.norm.running_var\n",
      "(512,)\n",
      "res3.0.shortcut.norm.weight\n",
      "(512,)\n",
      "res3.1.conv1.norm.bias\n",
      "(128,)\n",
      "res3.1.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.1.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.1.conv1.norm.weight\n",
      "(128,)\n",
      "res3.1.conv2.norm.bias\n",
      "(128,)\n",
      "res3.1.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.1.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.1.conv2.norm.weight\n",
      "(128,)\n",
      "res3.1.conv3.norm.bias\n",
      "(512,)\n",
      "res3.1.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.1.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.1.conv3.norm.weight\n",
      "(512,)\n",
      "res3.1.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.1.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.1.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.2.conv1.norm.bias\n",
      "(128,)\n",
      "res3.2.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.2.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.2.conv1.norm.weight\n",
      "(128,)\n",
      "res3.2.conv2.norm.bias\n",
      "(128,)\n",
      "res3.2.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.2.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.2.conv2.norm.weight\n",
      "(128,)\n",
      "res3.2.conv3.norm.bias\n",
      "(512,)\n",
      "res3.2.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.2.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.2.conv3.norm.weight\n",
      "(512,)\n",
      "res3.2.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.2.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.2.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res3.3.conv1.norm.bias\n",
      "(128,)\n",
      "res3.3.conv1.norm.running_mean\n",
      "(128,)\n",
      "res3.3.conv1.norm.running_var\n",
      "(128,)\n",
      "res3.3.conv1.norm.weight\n",
      "(128,)\n",
      "res3.3.conv2.norm.bias\n",
      "(128,)\n",
      "res3.3.conv2.norm.running_mean\n",
      "(128,)\n",
      "res3.3.conv2.norm.running_var\n",
      "(128,)\n",
      "res3.3.conv2.norm.weight\n",
      "(128,)\n",
      "res3.3.conv3.norm.bias\n",
      "(512,)\n",
      "res3.3.conv3.norm.running_mean\n",
      "(512,)\n",
      "res3.3.conv3.norm.running_var\n",
      "(512,)\n",
      "res3.3.conv3.norm.weight\n",
      "(512,)\n",
      "res3.3.conv1.weight\n",
      "(128, 512, 1, 1)\n",
      "res3.3.conv2.weight\n",
      "(128, 128, 3, 3)\n",
      "res3.3.conv3.weight\n",
      "(512, 128, 1, 1)\n",
      "res4.0.conv1.norm.bias\n",
      "(256,)\n",
      "res4.0.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.0.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.0.conv1.norm.weight\n",
      "(256,)\n",
      "res4.0.conv2.norm.bias\n",
      "(256,)\n",
      "res4.0.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.0.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.0.conv2.norm.weight\n",
      "(256,)\n",
      "res4.0.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.0.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.0.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.0.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.0.conv1.weight\n",
      "(256, 512, 1, 1)\n",
      "res4.0.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.0.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.0.shortcut.weight\n",
      "(1024, 512, 1, 1)\n",
      "res4.0.shortcut.norm.bias\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.running_mean\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.running_var\n",
      "(1024,)\n",
      "res4.0.shortcut.norm.weight\n",
      "(1024,)\n",
      "res4.1.conv1.norm.bias\n",
      "(256,)\n",
      "res4.1.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.1.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.1.conv1.norm.weight\n",
      "(256,)\n",
      "res4.1.conv2.norm.bias\n",
      "(256,)\n",
      "res4.1.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.1.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.1.conv2.norm.weight\n",
      "(256,)\n",
      "res4.1.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.1.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.1.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.1.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.1.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.1.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.1.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.2.conv1.norm.bias\n",
      "(256,)\n",
      "res4.2.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.2.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.2.conv1.norm.weight\n",
      "(256,)\n",
      "res4.2.conv2.norm.bias\n",
      "(256,)\n",
      "res4.2.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.2.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.2.conv2.norm.weight\n",
      "(256,)\n",
      "res4.2.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.2.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.2.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.2.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.2.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.2.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.2.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.3.conv1.norm.bias\n",
      "(256,)\n",
      "res4.3.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.3.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.3.conv1.norm.weight\n",
      "(256,)\n",
      "res4.3.conv2.norm.bias\n",
      "(256,)\n",
      "res4.3.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.3.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.3.conv2.norm.weight\n",
      "(256,)\n",
      "res4.3.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.3.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.3.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.3.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.3.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.3.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.3.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.4.conv1.norm.bias\n",
      "(256,)\n",
      "res4.4.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.4.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.4.conv1.norm.weight\n",
      "(256,)\n",
      "res4.4.conv2.norm.bias\n",
      "(256,)\n",
      "res4.4.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.4.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.4.conv2.norm.weight\n",
      "(256,)\n",
      "res4.4.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.4.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.4.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.4.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.4.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.4.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.4.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res4.5.conv1.norm.bias\n",
      "(256,)\n",
      "res4.5.conv1.norm.running_mean\n",
      "(256,)\n",
      "res4.5.conv1.norm.running_var\n",
      "(256,)\n",
      "res4.5.conv1.norm.weight\n",
      "(256,)\n",
      "res4.5.conv2.norm.bias\n",
      "(256,)\n",
      "res4.5.conv2.norm.running_mean\n",
      "(256,)\n",
      "res4.5.conv2.norm.running_var\n",
      "(256,)\n",
      "res4.5.conv2.norm.weight\n",
      "(256,)\n",
      "res4.5.conv3.norm.bias\n",
      "(1024,)\n",
      "res4.5.conv3.norm.running_mean\n",
      "(1024,)\n",
      "res4.5.conv3.norm.running_var\n",
      "(1024,)\n",
      "res4.5.conv3.norm.weight\n",
      "(1024,)\n",
      "res4.5.conv1.weight\n",
      "(256, 1024, 1, 1)\n",
      "res4.5.conv2.weight\n",
      "(256, 256, 3, 3)\n",
      "res4.5.conv3.weight\n",
      "(1024, 256, 1, 1)\n",
      "res5.0.conv1.norm.bias\n",
      "(512,)\n",
      "res5.0.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.0.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.0.conv1.norm.weight\n",
      "(512,)\n",
      "res5.0.conv2.norm.bias\n",
      "(512,)\n",
      "res5.0.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.0.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.0.conv2.norm.weight\n",
      "(512,)\n",
      "res5.0.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.0.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.0.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.0.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.0.conv1.weight\n",
      "(512, 1024, 1, 1)\n",
      "res5.0.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.0.conv3.weight\n",
      "(2048, 512, 1, 1)\n",
      "res5.0.shortcut.weight\n",
      "(2048, 1024, 1, 1)\n",
      "res5.0.shortcut.norm.bias\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.running_mean\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.running_var\n",
      "(2048,)\n",
      "res5.0.shortcut.norm.weight\n",
      "(2048,)\n",
      "res5.1.conv1.norm.bias\n",
      "(512,)\n",
      "res5.1.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.1.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.1.conv1.norm.weight\n",
      "(512,)\n",
      "res5.1.conv2.norm.bias\n",
      "(512,)\n",
      "res5.1.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.1.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.1.conv2.norm.weight\n",
      "(512,)\n",
      "res5.1.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.1.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.1.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.1.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.1.conv1.weight\n",
      "(512, 2048, 1, 1)\n",
      "res5.1.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.1.conv3.weight\n",
      "(2048, 512, 1, 1)\n",
      "res5.2.conv1.norm.bias\n",
      "(512,)\n",
      "res5.2.conv1.norm.running_mean\n",
      "(512,)\n",
      "res5.2.conv1.norm.running_var\n",
      "(512,)\n",
      "res5.2.conv1.norm.weight\n",
      "(512,)\n",
      "res5.2.conv2.norm.bias\n",
      "(512,)\n",
      "res5.2.conv2.norm.running_mean\n",
      "(512,)\n",
      "res5.2.conv2.norm.running_var\n",
      "(512,)\n",
      "res5.2.conv2.norm.weight\n",
      "(512,)\n",
      "res5.2.conv3.norm.bias\n",
      "(2048,)\n",
      "res5.2.conv3.norm.running_mean\n",
      "(2048,)\n",
      "res5.2.conv3.norm.running_var\n",
      "(2048,)\n",
      "res5.2.conv3.norm.weight\n",
      "(2048,)\n",
      "res5.2.conv1.weight\n",
      "(512, 2048, 1, 1)\n",
      "res5.2.conv2.weight\n",
      "(512, 512, 3, 3)\n",
      "res5.2.conv3.weight\n",
      "(2048, 512, 1, 1)\n",
      "stem.conv1.weight\n",
      "(64, 4, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from safetensors import safe_open\n",
    "import pickle\n",
    "\n",
    "mask2formerResnetPklPath = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/R-50.pkl'\n",
    "BigEarthResnetPthPath = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/model_S2.safetensors'\n",
    "\n",
    "# Keep the Bigearth weights in a dictionary with the keys from the Mask2Former model\n",
    "bigearth_weights = {}\n",
    "key_conv_stem = (\"model.vision_encoder.conv1.weight\",\"stem.conv1.weight\")\n",
    "\n",
    "with safe_open(BigEarthResnetPthPath, framework=\"pt\") as f:\n",
    "    for key in f.keys():\n",
    "        if key in dictionary_from_Sentinel_to_Mask2former:\n",
    "            new_key = dictionary_from_Sentinel_to_Mask2former[key]\n",
    "            bigearth_weights[new_key] = np.array(f.get_tensor(key))\n",
    "\n",
    "    value_stem = np.array(f.get_tensor(key_conv_stem[0]))\n",
    "    value_stem = value_stem[:,[8,4,3,2],:,:]\n",
    "    print(value_stem.shape)\n",
    "    bigearth_weights[key_conv_stem[1]] = value_stem\n",
    "\n",
    "for key in bigearth_weights.keys():\n",
    "    print(key)\n",
    "    print(bigearth_weights[key].shape)\n",
    "\n",
    "# Load the weights from the pickle file\n",
    "with open(mask2formerResnetPklPath, 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "\n",
    "\n",
    "# fix stems convolution channel\n",
    "\n",
    "\n",
    "\n",
    "weights[\"model\"] = bigearth_weights\n",
    "\n",
    "# Save the updated weights to a new pickle file\n",
    "new_pkl_path = '/home/pablo.canosa/wip/python_notebooks/ResnetWeightConversion/R-50_S2_correct_stem_bands.pkl'\n",
    "with open(new_pkl_path, 'wb') as f:\n",
    "    pickle.dump(weights, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_mask_alb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
